{"pages":[{"url":"netbeans-8-maven-lombok.html","text":"On a personal project recently I wanted to try using Project Lombok to cut down on java boilerplate. I found much of the online documentation for this either incomplete or just wrong, so I'm documenting what I did to get things (mostly) working here in case it helps someone else. Some of the common problems I ran into: The first recommendation in both lombok's setup page and NetBeans' own is to enable 'Enable Annotation Processing in Editor'. This option isn't available for maven projects, though. Working maven instructions such as this blog are for older versions of lombok, which were missing some features I wanted. More up-to-date blogs such as this one get close, but involve hacks like having to build your project twice each time you change a lombok-annotated class. Not ideal. Here's what I did: Add maven dependency for the latest lombok (I've marked it as having provided scope because I don't want lombok.jar in my uberjar): <dependency> <groupId> org.projectlombok </groupId> <artifactId> lombok </artifactId> <version> 1.16.8 </version> <scope> provided </scope> </dependency> Add the maven compiler plugin for lombok. Note that I disable adding the generated code to the maven build path; the reasons for this will be clear later: <build> <plugins> <plugin> <groupId> org.projectlombok </groupId> <artifactId> lombok-maven-plugin </artifactId> <version> 1.16.8.0 </version> <executions> <execution> <phase> generate-sources </phase> <goals> <goal> delombok </goal> </goals> <configuration> <sourceDirectory> src/main/java </sourceDirectory> <addOutputDirectory> false </addOutputDirectory> </configuration> </execution> </executions> </plugin> </plugins> </build> With this in place, lombok should be invoked at compile time and generate classes into target/generated-sources/delombok . However, those generated classes aren't compiled, because we prevented the directory being added to the build path. Why? Because if we try to compile both src/main/java and target/generated-sources/delombok we get tons of compile errors due to duplicate class definitions (because every single class exists in both locations). We do actually need to build the generated classes though, because they are the ones that have all the generated accessors, builders etc that the rest of the code depends on. It's not as simple as just changing the sourceDirectory though - if you do that, NetBeans will honour it and only show target/generated-sources/delombok in the editor, which means you can't edit your original source! We solve this with maven profiles. The idea is that we have one profile with sourceDirectory set to src/main/java , and tell NetBeans to use this profile when coding. Then, we have another profile with sourceDirectory set to target/generated-sources/delombok , and use that when compiling. Here's how. First set a property to contain the desired source location: <properties> <!-- set default source location --> <src.dir> src/main/java </src.dir> </properties> Then create a profile that switches the location: <profiles> <profile> <id> lombok-build </id> <properties> <src.dir> ${project.build.directory}/generated-sources/delombok </src.dir> </properties> </profile> </profiles> Finally, set the sourceDirectory based on this property: <build> <sourceDirectory> ${src.dir} </sourceDirectory> <plugins> ... </plugins> </build> Now we have src/main/java as the default location, so NetBeans will allow you to edit your source properly. Then, specify lombok-build as the active profile for various actions (build, debug etc) under Project Properties->Actions->Activate Profiles. When you build within NetBeans, it will run delombok and then compile the generated source rather than the original source. This works pretty well for me. It's not quite perfect - you have to remember to use the correct profile for builds outside NetBeans, e.g. from the command line or a CI server - but it's pretty close. I'm very much enjoying not having builders clogging up my code, or having to write endless toString methods. Lombok is fun!","tags":"tools","title":"NetBeans 8, Maven, and Project Lombok"},{"url":"yesterday-i-was-crap.html","text":"There's a very common meme in the tech world which has always struck me as the programmer equivalent of telling your classmates how easy you found that exam you all just took. It's stupid bravado and serves no purpose other than trying to make people feel bad. You'll recognise it as a variation on the following: If you look at the code you wrote 6 months ago and don't think it's crap, you're not growing as a programmer. It's bullshit, and I'm calling it out. Sure, you should improve over time. Sure, you might look at code you wrote 6 months ago and realise there are better ways to do it, but don't feel bad if you don't think it's crap. When I look back at old code I may see ways I could improve it, but it normally holds up OK given the older version of the language I was using, the now-obsolete libraries I depended on, the time constraints I was under. Give your younger self a break, not everything they did sucked.","tags":"commentary","title":"Yesterday I Was Crap"},{"url":"logging-connections-with-firewalld.html","text":"I was recently trying to diagnose a production connectivity issue on a CentOS 7 box and found it a bit non-obvious how to get the firewall to log connection attempts. It is in fact documented in section 4.5.15.4.3 (how about that for a document subsection?!) but for ease of reference I'm putting it here. Basically, add a rich rule that includes log level details. For example, to open port 10000 for IP address 198.51.100.0, use the following: $ sudo firewall-cmd --zone = public --add-rich-rule = \"rule family=\" ipv4 \" source address=\" 198 .51.100.0/32 \" port protocol=\" tcp \" port=\" 10000 \" log prefix=\" test-firewalld-log \" level=\" info \" accept\" Connection attempts from that IP address will then be logged in /var/log/messages: $ sudo tail -f /var/log/messages | grep test-firewalld-log","tags":"sysadmin","title":"Logging Connections With firewalld"},{"url":"postgresql-jdbc-client-certificates.html","text":"There is a distressing lack of info out there about configuring the postgresql JDBC driver to present a client certificate to a database server when using SSL. It appears that checking the server certificate works out of the box, but not the client certificate. In this post I am using the following software versions: PostgreSQL 8.1 (yes, old, I know) postgresql-9.4-1200-jdbc41 If you are using different versions then results may vary, but this should be fairly widely-applicable. The postgres JDBC docs hint that a custom SSLSocketFactory is required in order to establish an SSLContext instance that uses your client cert, but rather unhelpfully goes on to say they don't know how to do it: The Java SSL API is not very well known to the JDBC driver developers and we would be interested in any interesting and generally useful extensions that you have implemented using this mechanism. Specifically it would be nice to be able to provide client certificates to be validated by the server Yes, it would be nice. And indeed, someone already has - but it seems to be woefully undocumented. In the org.postgresql.ssl.jdbc4 package we find the LibPQFactory class, which handles everything we need - and, in what I personally consider a bonus, works with .crt files directly rather than mucking about with keystores and the perenially stupid keytool. By default, this class looks in ~/.postgresql/ for the files it needs to work, though you can override the location. You need a root.crt containing the CA for the server certificate, plus your client certificate ( postgresql.crt ) and private key ( postgresql.pk8 ). Assuming you are using the default locations, you can test the basic connection using psql: psql \"sslmode=verify-full host=<hostname> dbname=postgres user=<username>\" If all is well, you will be prompted for your password and a connection will be established. If not, things to look at include your postgres config (is there a hostssl line in pg_hba.config for your client host?) and your firewall. Although psql has no trouble, the JDBC driver has format restrictions. The private key must be PKCS8 and stored in DER format, whereas the certificate is fine in PEM format (because of course it is). If your key is in PEM format (i.e. starts with something like -----BEGIN PRIVATE KEY----- ) you can convert it with openssl: openssl pkcs8 -topk8 -inform PEM -outform DER -in postgresql.key -out postgresql.pk8 -nocrypt Note that this creates an unencrypted key file; leave off the -nocrypt parameter if you want it encrypted, and specify the password at runtime. Then, finally, specify your connection string like so: jdbc : postgresql ://< hostname >/ postgres ? ssl = true & sslfactory = org . postgresql . ssl . jdbc4 . LibPQFactory & sslmode = verify - full Note the sslmode parameter - this is one of a number of parameters that control behaviour of the factory. The full list is in the source; the interesting ones include sslcert , sslkey , and sslrootcert (overrides the location/name of root.crt , postgresql.crt and postgresql.pk8 ); and sslpassword (decryption password if you encrypted your private key).","tags":"coding","title":"PostgreSQL, JDBC, and Client Certificates"},{"url":"a-handy-settings-pattern.html","text":".Net has a fairly nice strongly-typed settings file , but unfortunately the most common pattern of access is the big fat static accessor Settings.Default . Whilst refactoring some old code recently to use dependency injection, I found myself struggling a bit to manage settings nicely so that this static wart worked correctly. I came up with the following little pattern which, so far, works well for me so I thought I'd share it. Firstly, in any project that needs runtime configuration, define an interface that contains readonly properties for the data it needs. namespace MyClassLibraryProject { public interface ISettings { string MyStringSetting { get ; } bool SomeFlag { get ; } } } Then in any class that depends on the settings, make sure the constructor accepts that interface (the DI container will inject it). namespace MyClassLibraryProject { public class MyConfigurableClass : IMyConfigurableClass { private readonly ISettings _settings ; public MyConfigurableClass ( ISettings settings ) { _settings = settings ; } } } Then, in whatever project builds your actual executable (e.g. a Console app, Windows Service, or even just your unit tests) define your Settings.settings file as normal, making sure that it contains settings names that match the properties you defined. This will generate a Settings class for you with the properties needed to implement the interface defined by your class library. namespace MyExecutableProject.Properties { [global::System.Runtime.CompilerServices.CompilerGeneratedAttribute()] [global::System.CodeDom.Compiler.GeneratedCodeAttribute( \"Microsoft.VisualStudio.Editors.SettingsDesigner.SettingsSingleFileGenerator\", \"12.0.0.0\")] internal sealed partial class Settings : global :: System . Configuration . ApplicationSettingsBase { private static Settings defaultInstance = (( Settings ) ( global :: System . Configuration . ApplicationSettingsBase . Synchronized ( new Settings ()))); public static Settings Default { get { return defaultInstance ; } } [global::System.Configuration.ApplicationScopedSettingAttribute()] [global::System.Diagnostics.DebuggerNonUserCodeAttribute()] [global::System.Configuration.DefaultSettingValueAttribute( \"some string value\")] public string MyStringSetting { get { return (( string )( this [ \"MyStringSetting\" ])); } } [global::System.Configuration.ApplicationScopedSettingAttribute()] [global::System.Diagnostics.DebuggerNonUserCodeAttribute()] [global::System.Configuration.DefaultSettingValueAttribute(\"False\")] public bool SomeFlag { get { return (( bool )( this [ \"SomeFlag\" ])); } } } } Importantly, this generated class is partial , so you can extend it. The extension doesn't need to do anything other that declare that it implements the settings interface. // ReSharper disable once CheckNamespace namespace MyExecutableProject.Properties { internal partial class Settings : MyClassLibraryProject . ISettings { } } Finally, in your composition root map the static accessor for this Settings class to the interface. In this example I'm using SimpleInjector . var container = new Container (); container . RegisterSingle < MyClassLibraryProject . ISettings >( Settings . Default ); container . Register < MyClassLibraryProject . IMyConfigurableClass , MyClassLibraryProject . MyConfigurableClass >(); SimpleInjector will now automatically inject Settings.Default into every instance of MyConfigurableClass , behind the ISettings interface. This of course makes it trivial to mock settings in tests to check configurable behaviour without having to muck around with a settings file. [Test] public void TestSomething () { var settings = Mock . Of < ISettings >( s => s . MyStringSetting == \"test with this value\" ); var sut = new MyConfigurableClass ( settings ); Assert . IsTrue ( sut . DoesTheRightThing ()); } As a bonus, because this is all strongly-typed, automated renames work as you expect. Rename a property on the interface and it will rename the property on the Settings class automatically (at least, Resharper does - not sure about vanilla Visual Studio).","tags":"coding","title":"A Handy Settings Pattern"},{"url":"windows-debugging-armoury.html","text":"Searching around the web will reveal a number of debugging setup guides. There are lots of little tips and tricks that you pick up through a career of figuring out why your production code is misbehaving, and it's helpful to jot it all down in one place. This is my toolkit. There are many like it, but this one is mine. More specifically, this is for dealing with .Net applications on Windows. I might create something similar for dealing with java on Linux at some point. Install List Windows Driver Kit 8.1 (requires VS2013). WDK 8 is no longer supported. SOSEX 4 Psscor4 Managed-Code Debugging Extension for WinDbg Alternatively, you can download my zip file of these tools. No installation needed, just copy it where you need it. Of course, if you don't trust me, get everything from source. And get permission from your friendly sysadmin before putting this stuff on a production box. Set-up Symbols Create directories on a disk with a couple of gigabytes free space: mkdir C:\\Symbols mkdir C:\\SymbolCache Create the following environment variables: _NT_SYMBOL_PATH=SRV*C:\\Symbols*http://msdl.microsoft.com/download/symbols _NT_SYMCACHE_PATH=C:\\SymbolCache Add any local/app symbols. For instance if you have an application and associated PDB files in C:\\temp\\PDB : C:\\> \"C:\\Program Files (x86)\\Windows Kits\\8.0\\Debuggers\\x86\\symstore\" add /f \"C:\\temp\\PDB\\*.*\" /s c:\\Symbols /t \"Debuggable Server\" C:\\> \"C:\\Program Files (x86)\\Windows Kits\\8.0\\Debuggers\\x86\\symstore\" query /s c:\\Symbols /f C:\\temp\\PDB\\Server.exe Add C:\\Program Files (x86)\\Windows Kits\\8.0\\Debuggers\\x86 to your path if you expect to use symstore frequently. For more information on symstore, check out the symstore docs . Also see Setting Yourself up for Debugging at Thomas Kejser's Database Blog. ETW Profiling Event Tracing for Windows is a low-level, low-impact form of system tracing that lies dormant until activated with either xperf or Windows Performance Recorder. It is analogous to dtrace on *nix systems. WPR/WPA Windows Performance Recorder in conjunction with Windows Performance Analyzer is an insanely powerful way of profiling performance of .Net applications running in production, without the overhead of more traditional code profilers. To look at kernel context switches (indicative of blocking calls and lock contention), open Computation -> CPU Usage (Precise) -> Context Switch Count by Process, Thread . Rearrange the columns so that NewProcessName , NewThreadStack , ReadyingProcess , and ReadyThreadStack are to the left of the thick yellow line. Sort descending by Waits (us) on the right. Select Load Symbols from the Trace menu. This will take a while, but once done you can drill down into your code and see exactly where threads are being switched back in and what happened to allow them to continue (e.g. which line of code was blocking, and which line of code unblocked it). With a bit of practice, this is like having the Eye of freakin' Sauron glaring at your code for you. Coarse-grained locks deep in the .Net framework itself are dragged kicking and screaming into the sunlight. Awful connection pool management in your database driver is held up for all to see. No-one escapes. Flame Graphs Flame graphs are a very useful visualisation of CPU usage broken down by stack trace. They were originally designed to process dtrace profiles, but Bruce Dawson wrote a pre-processor that converts xperf/WPR traces to a compatible format. Check out the linked blog posts for details. Note that you probably want to use WPA first to pin down short intervals of interest, as trying to generate a flamegraph of, say, 5 seconds duration on software doing 30k requests per second is a bit of a system killer to say the least. Debugging Production debugging is a tricky beast. If you have a route through the network and some off-peak time, you can connect with Visual Studio's remote debugger. This can kill performance though. For memory problems, you can just as usefully grab a process dump and debug it on your own workstation at your leisure. Start WinDbg Open crash dump file (Ctrl-D) Load sos.dll: .loadby sos clr Try running a SOS command, e.g. !threads . If it fails with a 'load data access DLL' error, it's probably the wrong version of SOS (even the revision numbers have to match). Follow the instructions and run .cordll -ve -u -l to check, and if necessary grab SOS.dll from the dump machine (typical path C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319 ) Load psscor4: .load < PATH_TO_PSSCOR4.DLL > Load sosex.dll: .load < PATH_TO_SOSEX.DLL > Set up symbol path. If you have a local symstore (as above), use: .sympath srv * c : \\ symbols * http : // msdl.microsoft.com / download / symbols If you just have an app directory containing PDBs, use: .sympath srv * http : // msdl.microsoft.com / download / symbols .sympath + \"C:\\Program Files\\DeployedServer\" Reload modules: .reload / f Toggle debug info with: !sym noisy !sym quiet Enable DML (hyperlinks symbols so you can navigate the object graph with the mouse): .prefer_dml 1 Further symbol path info Using WinDbg WinDbg is not what you'd call beginner-friendly. The following pages have some useful lists of commands in addition to those I've covered below. Common WinDbg Commands (Thematically Grouped) SOS.dll (SOS Debugging Extension) WinDbg cheat sheet Info Command Description Example x display symbols x clr!Thread::* Threads/Execution Command Description !threads Display all threads .shell -ci \"!threads\" findstr 15 pipes output of !threads into findstr , useful for e.g. looking up a managed thread ID (OSID) to find the thread ID that can be used with ~s ~22s switch debugger to thread ordinal 22 ~~[12AB]s switch to managed thread ID 0x12AB !pe dump exception on current thread !clrstack dump managed stack !mk dump managed and native stack !do dump object !sosex.dlk search for deadlocks !sosex.mlocks search for threads holding locks !sosex.mwaits search for threads waiting on locks !psscor4.syncblk run command for all threads !eestack -short -EE todo ~*e!\\<command\\> run command for all threads !psscor4.dumpallexceptions dumps every System.Exception or subclass on the heap !sosex.mframe set current frame for !mdv . Use !mk to identify frames !sosex.mdv display arguments and parameters for current stack frame Memory Command Description !dumpheap stat heap stats !dumpheap type MyAssembly.MyClass statistics heap for given type","tags":"tools","title":"Windows Debugging Armoury"},{"url":"visual-studio-logs.html","text":"Suffering from the accursed slow Visual Studio startup problem (which really shouldn't happen on the beefy i7 with 16GB RAM and SSD I use at work), I wanted to dig a little deeper and diagnose it. From stackoverflow : Run devenv /log and wait for Visual Studio to start up. Close the IDE to close the log/stop logging. Assuming VS2013, this will generate ActivityLog XML and XSL files in %APPDATA%\\Microsoft\\VisualStudio\\12.0\\ (adjust version number if you're using something other than VS2013). Open ActivityLog.xml in a browser, or open ActivityLog.xsl in VS itself and execute it ( [CTRL]+[ALT]+[F5] ), choosing ActivityLog.xml from the File Open dialog that pops up. This will give you a nicely-formatted HTML report where each step is timestamped, so look for the big gaps to find the things that are slowing you down (yes, it's probably Resharper).","tags":"tools","title":"Visual Studio Logs"},{"url":"death-to-confluence.html","text":"Way back in 2011, Atlassian made the somewhat surprising decision not to let you edit wiki markup in their wiki product, Confluence. Allegedly this was because people like 'sales and marketing' were too stupid to learn wiki syntax , and it was too hard to support both a rich text editor and a markup editor. This might be tolerable if the rich text editor were any good, but 3 years after declaring \"We know it's not perfect, yet \" it's still in this weird mutant state where you can type markup syntax into the editor, but it is instantly and irretrievably converted and then you can't edit it . Note: You cannot edit content in wiki markup . Confluence does not store page content in wiki markup. Although you can enter wiki markup into the editor, Confluence will convert it to the rich text editor format immediately. You will not be able to edit the wiki markup after initial entry . Huh? So, when I'm writing up a page that e.g. refers to a sproc or class name I tend to mark it up as preformatted text, as is conventional to denote code. I can type this in as {{my_sproc_name}} and it correctly gets converted to my_sproc_name . But, it's immediately and permanently converted. If I do something so stupid as to finish a paragraph with preformatted text and later want to go back and add something to the end in normal text, Confluence 'helpfully' thinks I want to continue writing in the preformatted block and I have to jump through hoops by rewriting the line and applying formatting again afterwards, or starting a new line in normal body text then deleting the line-break, etc. Who thought this was a good idea?","tags":"commentary","title":"Death To Confluence"},{"url":"3-way-merge-in-vim.html","text":"I have a nasty habit of forgetting certain commands when using vim to handle a 3-way merge, since the navigation is a bit non-standard. So, this is my (very quick) reference. Window layout shows the merged file at the bottom, the base (pre-conflict) file top-middle, and the diverging diffs either side. Cursor should be in the merged file which can be reached with standard window navigation (e.g. Ctrl-W j ). Previous conflict [c Next conflict ]c Then, to resolve a conflict, use the diffg command to select a winner. Choose local :diffg LO Choose remote :diffg RE Choose base :diffg BA Choosing base effectively reverts both diffs. If the conflict is too complex to fix by pulling a block, just rewrite as necessary in the merged file. More info here","tags":"tools","title":"3-Way Merge In Vim"},{"url":"blogger-wordpress-to-pelican.html","text":"Like all the cool kids these days, I've migrated my blog to use a static site generator. Pelican , in my case. There are lots of tutorials about this sort of thing, but I ran into a few things that needed additional tweaking so I'm documenting it here. Note this isn't exhaustive - you can find the basics of generating and publishing static blogs in the pelican docs. Importing Pelican will convert Wordpress xml exports, but not Blogger. No problem - there are Blogger to Wordpress converters available, so get your data into Wordpress export format. I used this one . Save the XML file in an export directory off the root of your directory layout, it can be a useful reference later. Comments I used Brian St Pierre's comments plugin to extract comments from my Wordpress export. I had to patch it to work with pelican 3.4, but my patch has now been pulled into the main code so it works fine. Links When using markdown I prefer to use reference-style links (where all the actual URLs are stored as footnotes at the end of the file). The initial import process creates inline links though, so I wanted to fix that. Local links Check your page interlinks to make sure the import process picked them up correctly. If not, edit them yourself to the following format: [my link text]({filename}/path/to/my-post.md) Note the {filename} macro - that's what pelican will use to generate your links in the final html. Converting to reflinks There's a nifty project called formd that can convert markdown links to inline or reference, or toggle between them. for FILE in $( find content -type f -not -path \"content/comments/*\" -iname '*.md' ) do formd -r < \" $FILE \" | sponge \" $FILE \" done Note the use of sponge to allow me to overwrite the original file with the reformatted one without needing an intermediate file. Get it with apt-get install moreutils . This is so useful I made it a fabric task: def reflinks ( f ): local ( 'formd -r < {0} | sponge {0}' . format ( f )) Then I can reprocess any file quickly like so: fab reflinks:content/My-Blog-Post.md Validating Somewhere in the process of converting content to markdown and links to references, sometimes links get garbled. Especially if they have characters that clash with markdown formatting, like parentheses and square brackets. There is an excellent, ancient tool called linklint written in perl (see, ancient) that can blast through your content, identify links, and hit them up to make sure they exist. It even follows redirects. linklint -quiet -no_anchors -doc linkdoc -root output -net /@ This also verifies local links, so it double checks the work pelican has done. It will generate a fairly spartan but very useful report in the linkdoc directory. Redirects Pelican will generate your output in a flat structure, rather than the year/month/day URL structure used by other blogging software. All your links will be correct, but incoming links from other sites will break which will devastate your ranking. Fortunately you can configure your webserver to issue permanent redirects to keep those links alive. First get a list of your old URLs from your export dump. I used powershell for this because I love how it handles XML, but use any tool you like. [xml] $content = get-content export / wordpress . xml $content . SelectNodes ( \"//link\" ) > export / oldurls . txt Next, construct your redirect rules on your webserver. In my case, I was moving everything from a subdomain to a directory, i.e. blog.basildoncoder.com to basildoncoder.com/blog , as well as the aforementioned URL change. So I have nginx rules like this: server { server_name blog.basildoncoder.com ; return 301 $ scheme : // basildoncoder . com / blog $ request_uri ; } That redirects all requests off of the blog subdomain. Then, in the main server block for basildoncoder.com, I have this rule: location ~ \"&#94;/blog/((\\d{2,4})/){1,3}\" { rewrite \"&#94;/blog/((\\d{2,4})/){1,3}(.*)$\" /blog/$3 permanent; } The regex there looks for between 1 and 3 occurrences of directories named with between 2 and 4 numbers, and captures the rest of the URL after it. So any of the following URLs will match, and capture the filename: /blog/2014/09/30/test.html /blog/2014/09/test.html /blog/2014/test.html Each of these will be permanently rewritten to /blog/test.html . Finally, bust out linklint again to check the old URLs all resolve correctly once the pages are deployed and the server configured: linklint @@oldurls.txt You'll get output like this: found 44 urls: ok ----- 88 urls: moved permanently (301) Linklint checked 44 urls: 44 were ok, 0 failed. 88 urls moved. (In this case there were twice as many moves as there are links because I redirect the subdomain then redirect the path.)","tags":"tools","title":"Blogger/Wordpress to Pelican"},{"url":"disabling-voice-control-on-ios-71.html","text":"I'm one of those people who feels irredeemably stupid if I try to talk to any piece of technology, so I duly turn off Siri on any iOS device I own. I recently discovered - via my 15-month-old son and his love of all things gadgety that he's not supposed to play with - that disabling Siri in iOS 7.1 (and probably older versions too) automatically enables Voice Control, which can be accessed from the Lock Screen by holding down the Home button. When I say 'discovered', what I actually mean is I found my son holding my phone with a faintly confused friend on the line trying to figure out why he was being gurgled at. It seems that 'da' uttered by an infant is recognised as 'Dan'. Who knew? Annoyingly, Voice Control cannot be disabled in iOS 7.1 - I ask you, what is the point of implementing fingerprint scanners and remote wipe technology and all that jazz if you are going to force-enable features that a toddler can use to bypass everything and call my mates at 6am? Anyway, the workaround I've discovered is to re-enable the thrice-damned Siri, then go back to Settings/Touch ID & Passcode and disallow access to Siri when locked. This is enough to make the Lock Screen a sufficient barrier for a small child. I can disable most of Siri's features to keep her quiet - she only shows up if I hold down the Home button after unlocking, which is at least significantly less likely than inquisitive little hands finding my phone on the couch and holding down the inviting button that makes the screen light up on the magic box for a couple of seconds. Technology is great, no?","tags":"misc","title":"Disabling Voice Control on iOS 7.1"},{"url":"git-command-to-open-branch-in-bitbucket.html","text":"George Brocklehurst has a useful little script for opening GitHub at a particular commit, right from the command line. I've found this handy enough in the past that I've created an equivalent for Bitbucket, which is what I use at work. #!/bin/bash bitbucket_remote = $( git remote -v | /c/Bin/grep -Eo 'bitbucket.org[:/][&#94;.]+' | head -1 ) if [ -z $bitbucket_remote ] then echo \"No bitbucket remote\" exit fi bitbucket_url = \"https:// ${ bitbucket_remote /:// } /\" if [ ! -z $1 ] then sha = $( git rev-parse $1 ) bitbucket_url = \" ${ bitbucket_url } commits/ $sha \" fi start $bitbucket_url Installation instructions are the same as George's - name the script git-bb and drop it on your PATH, then invoke it as a subcommand: $ git bb","tags":"tools","title":"Git command to open branch in Bitbucket"},{"url":"concurrentdictionary-getoradd-vs.html","text":"Some recent performance profiling of a server application under load turned up an interesting gotcha in .Net's ConcurrentDictionary that I think is well- worth knowing about. As you probably know, ConcurrentDictionary strives for performance in concurrent environments by using more sophisticated locking strategies than simply lock(this) . Briefly, it has a lock per bucket, which means that parallel writes are possible where the updates are in different buckets. A few operations acquire all locks, which obviously is a source of contention, but typically these are operations like Count and IsEmpty that shouldn't be called frequently. Finally, and of most relevance to this post, it supports lock-free reads which are the holy grail for high-concurrency. Except when it doesn't. ConcurrentDictionary has two overloads of the GetOrAdd method, an atomic operation to get a value if it exists, or add it if not. In the case where the value does not exist, you can either provide the new value directly, or pass a factory delegate that ConcurrentDictionary will call if it needs the value. The Remarks section in the MSDN docs warns that the factory delegate will be called outside the lock, but otherwise the two overloads appear identical functionally - in particular that \"read operations on the dictionary are performed in a lock-free manner\". This is not true, however, at least as of .Net 4.5. The factory delegate overload will call the lockless TryGetValue and only attempt to do an Add (thus acquiring a lock) if the read fails - this is sane and how I would expect it to be implemented. The non-delegate overload of GetOrAdd, however, does not call TryGetValue and will obtain the bucket lock before it attempts to read the key, meaning that it is no longer a lockless read! If you have lots of threads attempting to read from a ConcurrentDictionary with a small number of keys, you'll get massive lock contention with this method. Frankly I think this should be considered a bug since in a highly- concurrent environment this is a fundamental difference in behaviour for two methods which should be near-identical. I won't post any code here because I don't want to get into any licencing issues, but if you have Resharper or dotPeek you can easily decompile the framework and verify this behaviour yourself. Update : Now that the code is open source, I've been able to fix this and get it merged . Hurrah for open source!","tags":"coding","title":"ConcurrentDictionary: GetOrAdd vs GetOrAdd - One Of These Things Is Not Like The Other"},{"url":"mysql-table-not-found-error-when.html","text":"Note to self - the following bullet point from the MySQL docs applies to new tables as well as modified tables. The trigger cache does not detect when metadata of the underlying objects has changed. If a trigger uses a table and the table has changed since the trigger was loaded into the cache, the trigger operates using the outdated metadata. MySQL docs Do not hot-deploy a new table and a trigger update to refer to it, bad things will happen.","tags":"coding","title":"MySQL 'Table not found' Error When Referring To A New Table In A Trigger"},{"url":"hackish-monitoring-with-python-and.html","text":"I recently started at a new role and was asked, as a way to get a handle on part of the codebase, to add some logging and monitoring to a key service that didn't really tell the world too much about what it was doing - and when it did speak up, no-one was listening. Oh, and could you come up with something by tomorrow please? That would be great. Naturally it takes longer than a day to instrument an entire app, but since exceptions were being written to the event log I figured we could at least get some useful stuff from there. So the short-term challenge became finding a way to collect the event logs from 12 production servers, process them to figure out what's been happening, and then do it frequently enough that pretty graphs can be generated. This of course rules out doing anything with the MMC snap-in, as manually connecting to 12 boxes every few minutes would be a full-time job in itself. Powershell seemingly provides a number of possibilities, but I couldn't get Get-WmiObject queries to work, Get-EventLog wouldn't allow me to provide authentication credentials for the remote machine, and Get- WinEvent almost worked but failed to return the actual log message, even if I fiddled with the locale . I want to like Powershell, I really do, but every time I want to use it I hit bugs or OS compatibility issues. Recent Windows versions come with a handy tool called wevtutil , however, which is just what I needed. The following command does exactly what I want: wevtutil qe Application /c:50 /q:\"*[System[Provider[@Name='APP_NAME'] and (Level=1 or Level=2) and TimeCreated[timediff(@SystemTime)<300000]]]\" /e:Events /r:REMOTE_IP /u:SECURE\\\\Administrator /f:xml That gets me a maximum of 50 log entries from the last 5 minutes (300000ms) in the Application log with the specified log provider, on the specified machine, in XML format. Phew! From here, it's fairly simple to write some python to invoke that command, parse the XML response (so that errors can be categorised, e.g. database.timeout or network.connectivity), and fire some numbers off to the wonderful statsd . Then schedule the script to run every 5 minutes, and we have some very ghetto error monitoring in almost no time! Now begins the larger task of adding more detailed diagnostics to the app for more effective monitoring. Thanks to this page and this one for ideas on scripting wevtutil.","tags":"tools","title":"Hackish Monitoring With Python and wevtutil"},{"url":"marshalling-variable-length-array-from.html","text":"I recently spent time working on some C# code to interact with a simple DNS-SD system. This requires using DNS TXT records , which are not supported in the System.Net.Dns class. After a few google searches failed to turn up a pure .Net client library that met my needs, I settled on an approach based around p/invoking the Win32 DnsQuery function. And quickly ran into problems. For DNS TXT records, DnsQuery returns a DNS_TXT_DATA structure in the Data field of the DNS_RECORD structure. DNS_TXT_DATA is declared like this: typedef struct { DWORD dwStringCount ; PWSTR pStringArray [ 1 ]; } DNS_TXT_DATA , * PDNS_TXT_DATA ; Using the very handy P/Invoke Interop Assistant , we see that this struct can be represented like this in managed code: [StructLayout(LayoutKind.Sequential)] public struct DNS_TXT_DATA { /// DWORD->unsigned int public uint dwStringCount ; /// PWSTR[1] [MarshalAs(UnmanagedType.ByValArray, SizeConst=1, ArraySubType=UnmanagedType.SysUInt)] public IntPtr [] pStringArray ; } There is a problem with pStringArray, unfortunately. The System.Runtime.InteropServices.Marshal class cannot marshal a variable length array, as it needs to know in advance how big the array is in order to allocate memory. That's why the managed structure needs SizeConst specified in the MarshalAs attribute. However, if the DNS TXT record data contains multiple quoted strings separated by whitespace, DnsQuery will return a structure with a variable number of elements in pStringArray. Since SizeConst is set at compile-time, when we marshal this into the managed struct defined above, we only get the first element in our single-element array. Rats. More googling turned up very little info on dealing with this, though I found indications that others had run into the same problem without finding a satisfactory conclusion. DnsQuery is not the only Win32 function that returns variable-length arrays, and p/invoking any of the others has the same issue. Simply declaring SizeConst to be bigger than we need - \"hey, I know I'll never get more than 10 or so strings back, so why not declare SizeConst to be 128?\" - is inelegant (hardcoded upper limits, ugh) and doesn't work properly anyway. Since the struct layout is sequential the marshaller will copy over (e.g.) 128*sizeof(IntPtr) sequential bytes (a total of 512 bytes, in this case). That much memory was never allocated on the unmanaged side, so we end up with a load of junk in the tail of pStringArray, and more often than not the marshaller chokes on this junk and throws an AccessViolationException. Fun. There IS a way to get round the problem, though. I'm not sure it's the best way, but it works and seems stable, so I thought I'd throw it out there in case anyone else can use it (or maybe explain to me why it's an unsafe stupid thing to do...) Basically, since we're dealing with sequential memory, we can use Marshal.PtrToStructure to marshal the DNS_TXT_DATA structure as defined above, then use pointer arithmetic to gain access to any further data that needs marshalling. Pointer arithmetic? Oh yes, even in the safe and secure world of managed code it's sometimes still necessary to get our hands dirty, and situations like this illustrate that it will always be valuable to have some hard-earned Assembly/C/C++ war wounds. So, assuming we have valid p/invoke declarations and data structures (I've included a complete source program below), DnsQuery is called like so: var pServers = IntPtr . Zero ; var ppQueryResultsSet = IntPtr . Zero ; var ret = DnsQuery ( domain , DnsRecordType . TEXT , DnsQueryType . STANDARD , pServers , ref ppQueryResultsSet , IntPtr . Zero ); if ( ret != 0 ) throw new ApplicationException ( \"DnsQuery failed: \" + ret ); If we examine the memory location of ppQueryResultsSet (Ctrl-Alt-M,1 or Debug->Windows->Memory->Memory1 in Visual Studio) we'll see something like the following (actual address locations may vary - just copy the int value of ppQueryResultsSet to the Address bar of the memory window): 0x049E0878 00 00 00 00 .... 0x049E087C b8 09 9e 04 ¸.ž. 0x049E0880 10 00 20 00 .. . 0x049E0884 19 30 00 00 .0.. 0x049E0888 00 00 00 00 .... 0x049E088C 00 00 00 00 .... 0x049E0890 06 00 00 00 .... 0x049E0894 b8 08 9e 04 ¸.ž. 0x049E0898 d8 08 9e 04 Ø.ž. 0x049E089C f8 08 9e 04 ø.ž. 0x049E08A0 28 09 9e 04 (.ž. 0x049E08A4 68 09 9e 04 h.ž. 0x049E08A8 88 09 9e 04 ˆ.ž. I've set the column size to 4 here, as most of the values we are dealing with are 4 bytes in size. This effectively shows one value per line. The first 6 rows (24 bytes) correspond to the DNS_RECORD structure up until (but not including) the DNS_TXT_DATA structure in DNS_RECORD's Data union. We can marshal this first structure without problem: var dnsRecord = ( DnsRecord ) Marshal . PtrToStructure ( ppQueryResultsSet , typeof ( DnsRecord )); The DNS_TXT_DATA structure starts at address 0x049E0890 in my example. Having already marshalled the DNS_RECORD structure, now I want a pointer to the DNS_TXT_DATA structure. I can do this by creating a new pointer at the address of ppQueryResultsSet plus 24 bytes, and marshalling again: var ptr = new IntPtr ( ppQueryResultsSet . ToInt32 () + Marshal . SizeOf ( dnsRecord )); var txtData = ( DNS_TXT_DATA ) Marshal . PtrToStructure ( ptr , typeof ( DNS_TXT_DATA )); Because of the definition of DNS_TXT_DATA, this only marshals 8 bytes - 4 bytes for dwStringCount, and 4 bytes for the single element in pStringArray (an IntPtr). Since we know the memory is sequential, however, this gives us everything we need - we now know how many strings have been received (6 in this case, as indicated at 0x049E0890), and the location of the pointer to the first string (0x049E0894). With this info, we can marshal all the pointers into an array with a length of dwStringCount: ptr = new IntPtr ( ptr . ToInt32 () + sizeof ( uint )); // move to first var ptrs = new IntPtr [ txtData . dwStringCount ]; // dest array Marshal . Copy ( ptr , ptrs , 0 , ptrs . Length ); And finally we iterate through those pointers, marshalling the string pointed at by each: var strings = new List < string >(); for ( var i = 0 ; i < ptrs . Length ; ++ i ) { strings . Add ( Marshal . PtrToStringAnsi ( ptrs [ i ])); } While the example I've presented here is specific to DnsQuery, the general approach should be applicable to any situation where you need to marshal a data structure containing a variable-length array. Source code","tags":"coding","title":"Marshalling a Variable-Length Array From Unmanaged Code In C#"},{"url":"project-euler-problem-8.html","text":"Problem 8 \"Find the greatest product of five consecutive digits in the 1000-digit number. 73167176531330624919225119674426574742355349194934 96983520312774506326239578318016984801869478851843 85861560789112949495459501737958331952853208805511 12540698747158523863050715693290963295227443043557 66896648950445244523161731856403098711121722383113 62229893423380308135336276614282806444486645238749 30358907296290491560440772390713810515859307960866 70172427121883998797908792274921901699720888093776 65727333001053367881220235421809751254540594752243 52584907711670556013604839586446706324415722155397 53697817977846174064955149290862569321978468622482 83972241375657056057490261407972968652414535100474 82166370484403199890008895243450658541227588666881 16427171479924442928230863465674813919123162824586 17866458359124566529476545682848912883142607690042 24219022671055626321111109370544217506941658960408 07198403850962455444362981230987879927244284909188 84580156166097919133875499200524063689912560717606 05886116467109405077541002256983155200055935729725 71636269561882670428252483600823257530420752963450\" The first step here is to find a representation for that fairly humungous number. Obviously it's not going to fit into a paltry 32-bit int...but then we don't need it to. The problem description requires us to think in terms of smaller (5-digit) numbers, not one giant 1000-digit number. So, it is sufficient for us to consider the number as an enumerable stream of single digits, which we can conveniently represent as IEnumerable<int> . I could use a macro to convert the number into a collection initialiser, but it's much easier to treat the string as an IEnumerable<char> and let LINQ do the heavy lifting. var nums = Enumerable . AsEnumerable ( \"73167176531330624919225119674426574742355349194934\" + \"96983520312774506326239578318016984801869478851843\" + // ..... etc etc ...... \"05886116467109405077541002256983155200055935729725\" + \"71636269561882670428252483600823257530420752963450\" ). Select ( x => Convert . ToInt32 ( x . ToString ())); This gives us an IEnumerable<int> containing every digit in the 1000-digit number. Now, the 'obvious' way to solve the problem is to iterate through the collection, and at each index multiply the value against the next four indexes. A simple loop should deal with it: private static int SimpleSolver ( int [] ints ) { int max = 0 ; for ( int i = 0 ; i < ints . Length - 4 ; i ++) { int tmp = ints [ i ] * ints [ i + 1 ] * ints [ i + 2 ] * ints [ i + 3 ] * ints [ i + 4 ]; max = Math . Max ( max , tmp ); } return max ; } As ever, though, that's pretty ugly - the loop condition and product calculation is tied to the sequence size of 5, and messing with an index variable is tedious. An alternative approach is to take advantage of LINQ's Skip and Take methods to split the problem domain into overlapping 'slices'. Similar to the for loop above, the core of the approach is to iterate through the digits, and at each digit grab a number of subsequent digits and calculate the product. Lets look at the 5-digit slices available from the first 10 digits: 7 3 1 6 7 1 7 6 5 3 | 73167 | | 31671 | | 16717 | | 67176 | | 71765 | | 17653 | We can use Skip to progressively move the starting index forward, and Take to grab the 5 digits we need. So, starting with i=0, each successive slice can be sliced from the whole with: var slice = ints . Skip ( i ++). Take ( 5 ); To calculate the product of the digits in the slice, we can use the Aggregate operation: slice . Aggregate ( 1 , ( curr , next ) => curr * next ); We've met Aggregate before - it's basically a fold, which collapses a sequence to a single item by repeatedly applying an operation to an accumulating result. This can all be wrapped up as an iterator block, like so: private static IEnumerable < int > EnumerateSlices ( IEnumerable < int > ints , int sliceSize ) { int i = 0 ; while ( true ) { var slice = ints . Skip ( i ++). Take ( sliceSize ); if ( slice . Count () < sliceSize ) yield break ; // end yield return slice . Aggregate ( 1 , ( curr , next ) => curr * next ); } } Note the termination condition - when we have enumerated every slice, our next slice will contain only 4 elements (3, 4, 5, and 0 from the end of the sequence) - that's our cue to exit the loop. Also note that this approach makes the algorithm trivial to parameterize - it will work just as well with slice sizes other than 5. This iterator will produce an IEnumerable<int> containing the products of all slices, so the final step is to select the largest: var result = EnumerateSlices ( nums , 5 ). Max ();","tags":"projecteuler","title":"Project Euler Problem 8"},{"url":"killing-and-reviving-aspire-one.html","text":"I just spent 2 hours reviving my Aspire One netbook after inadvertently killing it whilst fiddling about configuring dropbox . I found the whole process unnecessarily fiddly and information on the interwebs to be a bit scarcer than I would have liked, so I'm documenting it here in case I need it in the future. Hopefully it'll be useful to someone else too. So, the cause of death was a typo when trying to set up the dropboxd daemon to start automatically on boot. I'm not running nautilus so couldn't use one of the prepackaged releases, and it's completely my fault that I made a mess of installing the vanilla x86 build. After making the fatal change and rebooting, the system would only boot up to a blank black screen with a default X mouse cursor. This is because the system was trying to run my broken command, failing, and therefore never getting to the main desktop. In the world of normal linux, there's all sorts of ways of dealing with this, but despite plenty of googling I couldn't find a way to use run-level 2 or 3 on an Aspire One, and the Ctrl+Alt+F1-F6 key combos for switching away from X to a terminal don't work either. There seems to be no way of preventing the system following the same doomed process over and again if you break X. Frustrated, I thought about using the restore disk, but that's a nuclear option - it re-paves the whole machine, so bye-bye data. That seemed a bit drastic when all I needed to do was edit a single text file to fix the system. Ironically, this was happening as a result of me trying to install a file sync system as a simple backup. Grr. Still, like countless thousands before me, I was saved by a live linux distro - in this case, a USB bootable one (since the Aspire One has no optical drive). Following the instructions 1 at pendrivelinux I created a bootable Feather Linux USB drive, and booted the netbook from it by hitting F12 on the post screen and selecting to boot from the USB stick. At the boot prompt, I used 'knoppix 3' to boot the system up to a command line, mounted /dev/hdc1 as an ext2 filesystem, and fixed my typo. Reboot, and tada! Everything was working again (well, after hitting Fn-F7 to reenable the touchpad, which I had accidentally disabled whilst mashing the keyboard in frustration at the sight of a blank screen about an hour earlier, heh). Note that I had to use a newer version of syslinux than the one referenced on pendrivelinux. This one worked for me. ↩","tags":"misc","title":"Killing and Reviving an Aspire One"},{"url":"macros-you-oughta-know.html","text":"One of the most useful tools available in any decent text editor is the macro recorder, but it's criminally underused. It seems most people either don't know the functionality exists, or simply ignore it. This is a shame, since it's a great timesaver. I don't know why macros are so underused. It might be a mindset thing - it can take a little while to develop the ability to spot repetitive editing tasks quickly (i.e. not when you're 75% of the way through thinking dang, I have to do this again? ), so maybe many people never quite make the leap. It's worth it though, because once you get your eye in you see chances to use macros everywhere. I had a useful example just yesterday, in which I needed to make a change to a colossal switch statement (220 branches! Run the cyclomatic-complexity doohickey on THAT!) and had no unit tests to fall back on. If I had to modify (and hopefully refactor) such a huge construct I wanted to be able to compare before-and-after test results, but I didn't much fancy hand-cranking a few hundred unit tests. By recording a temporary macro, however, it took just a couple of minutes to cover every branch. I've decided to post a detailed walkthrough of the process here in the hopes that a fairly simple example will be illustrative for those that don't already lean heavily on macros. Note that this is not an advanced tutorial . Please refrain from leaving snarky comments about how macros are so much more powerful than this - I'm just doing some introductory material here :-) Here is a representative snippet of the C# source. It's part of a legacy permissioning system that, under certain circumstances, needs to check for the existence of a permission represented by an enum against a permission table containing a string-based hierarchy (application/role/permission). The code I was modifying did the appropriate conversion: case PermissionKey . SecurityParameterManagementAdd : { return new string [] { \"Security\" , \"ParameterManagement\" , \"Add\" }; } case PermissionKey . SecurityRoleManagementAdd : { return new string [] { \"Security\" , \"RoleManagement\" , \"Add\" }; } case PermissionKey . SecurityRoleManagementModify : { return new string [] { \"Security\" , \"RoleManagement\" , \"Modify\" }; } case PermissionKey . SecurityUserManagementDelete : { return new string [] { \"Security\" , \"UserManagement\" , \"Delete\" }; } case PermissionKey . SecurityPermissionManagementAdd : { return new string [] { \"Security\" , \"PermissionManagement\" , \"Add\" }; } I needed to add a couple of branches to this, but I also wanted to tidy up the code by removing the superfluous braces, as a precursor to converting it into something a bit more robust and maintainable. I wanted unit test coverage to give me confidence that I hadn't mucked up some logic and inadvertantly granted admin access to the helpdesk trainee role or something. So, I copied the entire switch body into Notepad++ (well, vim really, but I'll pretend it's Notepad++ for the sake of making this post a bit more accessible) and set to work 1 . Before recording my macro, I needed to do a bit of preprocessing to trim the code down to just the data I wanted to work with. The following steps show the 'find' regexes I used (in each case, the value of the replace field was empty, so these are effectively deletes), and the effect on the first switch branch from the list above: Remove opening and closing braces from every switch branch: &#94;\\s+[\\{\\}]$ case PermissionKey . SecurityParameterManagementAdd : return new string [] { \"Security\" , \"ParameterManagement\" , \"Add\" }; Remove blanks - TextFX/Edit/Delete Blank Lines case PermissionKey . SecurityParameterManagementAdd : return new string [] { \"Security\" , \"ParameterManagement\" , \"Add\" }; Remove case statements and leading whitespace: &#94;\\s+case\\s+ PermissionKey . SecurityParameterManagementAdd : PermissionKey . SecurityParameterManagementAdd : return new string [] { \"Security\" , \"ParameterManagement\" , \"Add\" }; Remove colon from end of case statement: :$ Remove return statement and leading whitespace: &#94;\\s+return new string\\[\\]\\s* I ended up with a sequence of couplets looking similar to this one: PermissionKey . SecurityParameterManagementAdd { \"Security\" , \"ParameterManagement\" , \"Add\" }; Now the fun starts - lets walk through the process. We want to convert the first couplet into a simple unit test fixture, and record the process. This will be our macro - the instructions for converting one couplet into one unit test. We can then play the macro multiple times to convert all the others effortlessly. Start by moving the cursor to the start of the line, before the 'P' of PermissionKey. This is the start point of the macro, so for the macro to be repeatable we must make sure that we finish recording the macro in perfect position to run it again, i.e. before the 'P' of PermissionKey for the next couplet (column 0 line 3). Hit Ctrl-Shift-R to start recording. It is important not to use the mouse when editing - stick to the keyboard. It's also important not to record keystrokes that are too specific to one bit of code. For instance, don't use the arrow keys to move left and right character-by-character, because it won't work on longer or shorter lines. Instead, use the Home and End keys to jump to the start or end of the line, and hold Ctrl whilst arrowing left or right to move a word at a time instead of a character at a time (this is one of the areas where vim's movement commands really differentiate it from wannabes like Notepad++...but I digress). See the 'Detailed Instructions' section below for more information. Assume the original switch body is in a method called 'LookupEnumPermission'. The couplet should be edited to look like this (without the linewrap...): [Test] public void TestSecurityParameterManagementAdd () { string [] result = LookupEnumPermission ( PermissionKey . SecurityParameterManagementAdd ); Assert . AreEqual ( \"Security\" , result [ 0 ]); Assert . AreEqual ( \"ParameterManagement\" , result [ 1 ]); Assert . AreEqual ( \"Add\" , result [ 2 ]); } Make sure you finish by moving the cursor into position for the next couplet, and hit Ctrl-Shift-R again to stop recording. Now, hit Ctrl-Shift-P to play back the macro. If you've done everything right, the next couplet should magically format itself into a unit test. Hit Ctrl- Shift-P again, and the next couplet will change too. Under the Macro menu, select 'Run a macro multiple times...' and you can enter a fixed number of iterations, or just apply the macro over and over again until the end of the file is reached. Finally, you can copy the unit tests into a new or existing test fixture, and you're done! In much less time (hopefully) and with fewer errors than if the tests had been written one-by-one. Detailed Instructions: These are ley-by-key instructions in Notepad++, in case something in the description above is unclear. Visual Studio should be similar. Vim will be faster once you've learned how, but I'll assume if you use vim you're already au fait with this sort of editing :-) Type [Test], and hit enter to start a new line. Type 'public void Test' and hit Enter. Type '{' and hit Enter, then Tab. Hold Ctrl and tap the right arrow twice to jump over a couple of words and place the cursor at the start of the word SecurityParameterManagementAdd, then hold Ctrl-Shift and right arrow again to select the word. Ctrl-C to copy, then arrow up two lines and paste it after the word 'Test' to create the full function name TestSecurityParameterManagementAdd. Type () for the empty parameter list. Arrow down two lines and hit Home to jump to the start of the line. Type 'string[] result = LookupEnumPermission(', then hit End to jump to the end of the line and type ');'. Arrow down one line, hit Home, then Tab. Type 'Assert.Equals(' then hit Delete to remove the '{'. Hold Ctrl and move right three times (to move the cursor just past the comma) and type 'result[0]);' and hit Enter. Repeat variations of step 7 a couple of times to convert the next two lines. Remember to use the correct indexes (result[1] and result[2]). Hit Enter after the last line and type '}' to close the function body. Arrow down one line and hit Home to place the cursor at the correct start position for the next couplet, and end the macro by hitting Ctrl-Shift-R again. I could have just done this in a new file in Visual Studio, but for some reason I find VS intolerably slow at running macros once recorded. So slow, in fact, that you can watch the cursor laboriously complete each step - I wind up thinking it would have been quicker to do it manually. That might just be something odd about my VS installation though, as no-one else seems to think it's slow. ↩","tags":"tools","title":"Macros: You Oughta Know"},{"url":"ood-second-coming.html","text":"Over the last couple of months I've been burning up my free time on a pet project (hence the scarcity of posting here). This particular project is a web application, and since I've always been a desktop or middle-tier dude in my day job, it's a bit of a step out of my normal environment to grapple with browser compatibility and suchlike. Still, I wanted it to be a decent learning experience, so after a brief dalliance with Rails I scrapped the idea of using any framework sorcery and decided to write everything in plain ol' PHP , and lean heavily on YUI and jQuery to sort out the browser stuff. This probably isn't an approach I'd use in future projects, but I bet I'll appreciate those frameworks a lot more once I've encountered and understood the problems they attempt to solve. So, having strayed from the comfort of Rails and its clones, I had to think about lots of things like security, validation, data access, and how to organise my code. Just because I'd abandoned the training wheels I had no intention of falling over all the time - I still wanted a nice, maintainable app with sensible abstractions, properly decoupled, and resilient to failure. Time to start reading articles and the odd open source project, obviously. It's at this point I noticed something interesting. Since I regularly read plenty of development websites it could scarcely have escaped my notice that the trendy framework players (e.g. Rails, Django, Cake, ASP.NET MVC) strongly advocate the MVC pattern and class-based object-oriented design . What I hadn't really realised until now is how endemic that viewpoint had become. In fact, beyond a few admirably out-there frameworks like Seaside , it's almost universal. OOD = good, EVERYTHING ELSE = bad. MVC = good, EVERYTHING ELSE = bad. No shades of grey, no room for dissenting opinion. Go anywhere where best-practices are discussed and mention you're writing some procedural code, and watch the fireworks. It doesn't matter if your application has fewer lines of code than a newly-created Rails app has source files - if you haven't structured it with models, views, and controllers you may as well have written it in Visual Basic for all the bile you're going to have thrown at you. If you say you're writing functional code, you might get away with it, since functional programming is still widely misunderstood and you'll likely be classified as some weird LISPer or Schemer doing something arcane and thus ignored. Ironically, of course, if you grab a random Rails/Django/Cake app from github or Google Code , there's a pretty fair chance that what you'll find isn't particularly object-oriented anyway. Hint - usage of the 'class' keyword does not an object-oriented design make. And sweet zombie Jesus, I've never seen such abuse of the singleton pattern. That's a sure sign someone doesn't 'get' OO - the singleton pattern is evil and basically a way to shoehorn globals into an application without admitting it to your friends. So, we have massive fanatical advocacy of a technique that will allegedly solve all your problems, coupled with large-scale real-world misunderstandings and misapplication. Does this remind anyone of anything? Say, for example, the last time OOD swept the world, panacea to all programming woes, about 20 or so years ago? Don't get me wrong, I'm not arguing that class-based OO itself is just a fad - it deserves its place as a paradigm alongside procedural, functional, parallel, and numerous others. It's the heralding of OO as the one true way that seems faddish. So, in the very best \"bah, humbug\" traditions I've written my app in unashamedly procedural PHP code. I don't mix my presentation and content. My data layer is decoupled and unit tested. Every bit of SQL is a parameterised query, to guard against injection. I don't have a single echo() statement containing any html tags. All my errors are exception based, and I don't have a single die() call anywhere. My average function size is about 10 lines, and my longest is about 20. I've no doubt that there's a legion of 15-year-old self-appointed geniuses ready to accuse me of inflicting yet more spaghetti code junk on the world just because \"find . -iname '*php' | xargs grep class\" comes up empty, but hey I'm OK with that. I'm writing my next app in brainf*ck using ed.","tags":"development","title":"OOD - The Second Coming"},{"url":"project-euler-problem-7.html","text":"Problem 7 By listing the first six prime numbers: 2, 3, 5, 7, 11, and 13, we can see that the 6th prime is 13. What is the 10001st prime number? Ah, what a nice, straightforward, unambiguous spec! If only business software specifications were so precise. Way back in problem 3 , I took a bit of a wander off-topic and built a prime generator in .Net using the Sieve of Eratosthenes . Armed with this, problem 7 should be easy, right? The sieve implementation generates an IEnumerable<long>, which is non-indexable (i.e. I can't just say Primes()[10001]), but I can take the first 10,001 and then ask for the last element, which will be the answer to the problem. There's a problem with this, however. The sieve requires an upper bound during initialisation. This means it's great for solving problems like \"generate all the primes less than 10,001\", but not so great at answering questions like \"what is the 10,001st prime number?\", since it requires foreknowledge of the upper bound. To illustrate the problem, I'll take a wild guess at the upper bound. I'm going to guess that the 10,001st prime number is less than 99,999. What happens? var sieve = new SieveOfEratosthenes ( 99999 ); var result = sieve . Primes (). Take ( 10001 ). Last (); This generates an answer of 99,991. If I enter this into the Project Euler website, however, it tells me the answer is wrong. Gah! What went wrong? A simple test reveals the problem: var sieve = new SieveOfEratosthenes ( 99999 ); var primes = sieve . Primes (). Take ( 10001 ); var count = primes . Count (); There's only 9,592 primes generated! As the docs for Take() state (emphasis mine): Take<TSourceenumerates source and yields elements until count elements have been yielded or source contains no more elements. Damn. So, looks like my 99,999 guess was too small - with that as an upper bound, the sieve only finds 9,592 primes, and I need the 10,001st. OK, I'll bump it up by an order of magnitude: var sieve = new SieveOfEratosthenes ( 999999 ); var result = sieve . Primes (). Take ( 10001 ). Last (); This gives me the correct answer. Not exactly a wonderful solution though; the idea of having to guess the upper bound is pretty horrendous, and if this was real code it wouldn't be particularly maintainable - what if the requirements changed and we had to find the n th prime, which happened to be >99,999? We'd have to guess again. Ugh. Worse, the sieve algorithm precomputes all the primes up to the specified upper bound, meaning that in the above approach I've asked the sieve to generate primes up to 999,999 (all 78,498 of them!) despite only needing 10,001. Not very efficient. Fortunately, the upper bound can be calculated separately. Where n > 8601, as in this case, we can use the following equation : $$p(n) < n (log_e n + log_e \\cdot log_e \\cdot n - 0.9427)$$ where p( n ) is the n th prime number. Alternatively, for flexibility in handling n <8601, we can use the less accurate $$(n) < n \\cdot log_e \\cdot log_e \\cdot n$$ which works for n 5 . We can easily precompute the answers for n <=5, or simply calculate on demand. The formula can be implemented on the sieve class, with a factory method to help when we want to use it: public static SieveOfEratosthenes CreateSieveWithAtLeastNPrimes ( int n ) { return new SieveOfEratosthenes (( long ) Math . Ceiling ( UpperBoundEstimate ( n ))); } private static double UpperBoundEstimate ( int n ) { return n * Ln ( n ) + n * ( Ln ( Ln ( n ))); } private static double Ln ( double n ) { return Math . Log ( n , Math . E ); } This leaves us with an overall solution like so: var sieve = SieveOfEratosthenes . CreateSieveWithAtLeastNPrimes ( 10001 ); var result = sieve . Primes (). Take ( 10001 ). Last (); This generates a total 10,018 primes, cutting the wasted effort from almost 70,000 superfluous primes to just 17, and takes around 20ms to execute on my machine. Plenty fast enough, I think. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"projecteuler","title":"Project Euler Problem 7"},{"url":"look-before-you-look-before-you-leap.html","text":"Generally, I try to avoid turning this blog into some sort of snark -fest about other programmers or blogs. I've disagreed with Jeff Atwood once or twice though, and so by posting this I'm probably straying a little close to the edge...but what the hell. A couple of days ago Coding Horror carried a fluff piece about how all developers should be marketers too. Predictably, the article soon got posted to proggit where it was ripped on by reddit's resident Jeff-haters , and even more predictably the comments were a mix of interesting insight and barely-concealed hate. Apparently some of them got up Jeff's nose a bit, and today he responded . The core of his rebuttal seems to be that you shouldn't trust what you read on blogs, and should verify everything yourself. True enough, I guess, if perhaps a bit impractical given the sheer amount of information out there. Then, however, Jeff goes on to give an example by referencing a compression benchmark he'd read on a blog and providing counter-analysis to show that the benchmark was wrong in claiming Deflate is faster than gzip. In doing so, much knowledge was gained. Or so we are told. The comment thread quickly becomes a goldmine of humour. Bugs in Jeff's benchmarking code (not resetting the stopwatch) meant that the durations were cumulative, not independent, with inevitable distortion of the results. Another commenter pointed out that gzip cannot possibly be faster than Deflate, since the gzip algorithm IS the Deflate algorithm plus some additional computation. \"gzip\" is often also used to refer to the gzip file format, which is: a 10-byte header, containing a magic number, a version number and a timestamp optional extra headers, such as the original file name, a body, containing a DEFLATE-compressed payload an 8-byte footer, containing a CRC-32 checksum and the length of the original uncompressed data ( Wikipedia ) With the benchmarking code fixed, we see that Deflate is indeed slightly faster than gzip. All of which leads to repeated quotations from Jeff about the community being smarter than him, and some drastic toning down of language in post-publication edits to the article. I read a cached version of the RSS feed, which is markedly different to the article currently live on codinghorror.com - \"on my box, GZip is twice as fast as Deflate\" becomes \"on my box, GZip is just as fast as Deflate\", \"Deflate is way slower. It's not even close\" becomes \"Deflate is nowhere near 40% faster\", etc. Anyone who's tackled a major performance problem will likely agree that profiling is a tremendously valuable technique that should always be applied before attempting to optimise (i.e. look before you leap). I think this little episode has highlighted a couple of important things to bear in mind, however: Profiling isn't a magic wand - if you use buggy profiling code, you are leading yourself up the garden path. Profiling is less useful when you can reason (in the mathematical sense) about the code. That involves understanding the algorithms you are dealing with . Gzip is Deflate plus a bit more processing - so unless that extra processing has a negative duration gzip must necessarily take longer. You don't need a profiler to work that out. Look before you look before you leap. Anyway, enough hatcheting from me, normal service will be resumed shortly.","tags":"commentary","title":"Look Before You Look Before You Leap"},{"url":"project-euler-problem-6.html","text":"Onwards to... Problem 6 The sum of the squares of the first ten natural numbers is, 1&#94;2&#94; + 2&#94;2&#94; + ... + 10&#94;2&#94; = 385 The square of the sum of the first ten natural numbers is, (1 + 2 + ... + 10)&#94;2&#94; = 55&#94;2&#94; = 3025 Hence the difference between the sum of the squares of the first ten natural numbers and the square of the sum is 3025 - 385 = 2640. Find the difference between the sum of the squares of the first one hundred natural numbers and the square of the sum. Bit of a disappointment, problem 6; it's too easy. It's rated as the third- easiest , i.e. easier than problems 3 , 4 , and 5 which I've already covered. In fact, for my money it's easier than problem 2 as well. Ah well, the difficulty ramps up soon enough, trust me. Here's the very simple python solution: sum_sq = sum ([ x * x for x in xrange ( 1 , 101 )]) sq_sum = sum ( xrange ( 1 , 101 )) ** 2 print sq_sum - sum_sq As you can see, it's pretty intuitive. You sum the squares, square the sum, and calculate the difference. The answer is basically in the description, you just have to scale up a little. There's not much else to say about this one. Even if I abandon the functional approach and write a straightforward imperative solution it's still very straightforward. In (deliberately non-idiomatic, so don't whine at me) ruby: sum_of_squares = 0 sum = 0 1 . upto 100 do | x | sum_of_squares += x * x sum += x end p ( sum * sum ) - sum_of_squares","tags":"projecteuler","title":"Project Euler Problem 6"},{"url":"magic-numbers-and-other-numerical.html","text":"There are many coding practices that are near-universally regarded as 'bad', yet somehow keep cropping up over and over again. Conditional-branch abuse (including, yes, gotos). Deep nesting. Cryptic variable names. Global variables. Tight coupling. Entangled business/presentation logic. I could go on. Why do we keep doing it? Convenience? Laziness? Tiredness? Is unreadable spaghetti code some sort of steady-state/equilibrium for code? Is it a natural consequence of the vague and squidgy limitations of our evolved monkey-brains? Or is well-designed code abhorred like a vacuum and naturally atrophies into the sort of shambles you dread seeing on your first day at a new job, unless well-intentioned and dedicated people actively work to clean and polish it, like the Forth Bridge ? I don't have the time or wit to give this subject the treatment it deserves, but I do want to rant a bit about another symptom of this disease, which has given me a couple of sleepless nights recently. I refer, as the title might suggest, to magic numbers . Magic numbers are constants, unnamed in the most pathological cases, that represent an assumption or a limit in a piece of code. They often cause problems because soon they are forgotten about or their meaning is lost - and then something happens to invalidate the assumption, the code breaks, and all hell breaks loose. Magic numbers, to stretch the definition a bit, can also be implicit. If you are using a 32-bit integer, your magic number is 2,147,483,647 - that's the biggest number you can store in that type. Often, movement up to and beyond these ranges can trigger long-dormant bugs that are no fun at all to diagnose. Three times in recent history I've been bitten by bugs of this class, triggered by auto-incrementing sequences in database. These are they: A table in a database had a 32-bit integer primary key. At the time this seemed like a perfectly reasonable default, but insanely fast growth in usage of the system meant that the ~2.1billion upper limit of that data type was quickly reached. The DB column was switched to a 64-bit integer, but some of the client applications reading that table were not identified as at-risk. When the sequence generator left the 32-bit range, those applications overflowed. This happened at 4:30pm on a Friday afternoon. Saturdays were peak-times for system usage. You can imagine the frantic hacking that ensued. A sequence generator for a particular entity was started at 20,000,000, so as not to clash with the ID sequence of a related entity (that had started at 0 a good few years earlier). The similarity between the entities and the need to not have the IDs overlap had valid business justification, but the magic number was selected arbitrarily and promptly forgotten. Inevitably, the latter sequence surpassed that number, causing bizarre and difficult-to-trace entity relationship corruption that manifested as strangely-disappearing data on the front-end. A stored procedure parameter was incorrectly declared as an OracleType.Float, when it should have been an OracleType.Int32. This resulted in the value being cast from an integer to a floating-point and back again. For the first 16,777,216 integers, this happens to work OK. For the value 16,777,217, however, the loss in precision means that the number changes during casting. This simple bit of (heavily contrived) code shows the problem: static void Main ( string [] args ) { for ( int i = 0 ; i < 17000000 ; ++ i ) { if ( i != ( int )( float ) i ) { Console . WriteLine ( \"{0} != {1}\" , i , ( int )( float ) i ); break ; } } } There are many numbers above 16,777,217 that have this characteristic; 16,777,217 just happens to be the first, for reasons you can probably divine if you think the IEEE floating-point spec is a riveting read. A couple of weeks after the launch of a fairly major internal application, this time-bomb exploded due to a sequence reaching the magic number. The bug was nothing to do with the new application, but of course fingers were pointed at it since a long-running and stable system had mysteriously choked very shortly after deployment of the new application. Now, unquestionably, all these problems are avoidable, and a strong argument could be made that none of them should ever have been allowed to happen. Yet, for many reasons, they do. For example, first-mover advantage can mean the opportunity cost of taking the time to do things right first time is greater than the cost of fixing problems later. Also, people make assumptions. The issue underlying the Millennium Bug hysteria was caused by well- meaning developers who knew that two-digit dates wouldn't work after 1999 (effectively another magic number), but assumed the software would have been replaced or upgraded by then. No doubt that seemed a totally reasonable assumption in the 1970s, and it had genuine technical benefits (storage space was so tight that every byte saved was a battle won). Anyway, I don't have a magic bullet solution for this, I'm just venting spleen. Unit tests can help, but won't magically eliminate this class of bug (no matter what some of the more extreme TDD fanatics might tell you), so I suppose the lesson to take from this is the importance of being able to recognise and diagnose potential magic number issues. Pay close attention to data types, type conversions, and current values of sequences in your database. Keeping a sacrifical goat on hand might pay dividends too, in case any blood-thirsty deities with a head for binary arithmetic are watching.","tags":"development","title":"Magic Numbers and Other Numerical Nightmares"},{"url":"ubuntu-xmonad-and-ode-to-apt.html","text":"This weekend I finally got around to updating my main Linux box from Ubuntu 7.10 to 8.04 (yes, I know, 4 months late - but moving fast!). The highly excellent xmonad has made it into the main Ubuntu repositories, so I discarded my own build and grabbed the packaged version - which promptly didn't work as expected on my dual-head setup. Gah. A bit of googling suggested that the problem lay with the upstream debian package, which contained a build of libghc6-x11-dev that was compiled without xinerama support. This left me with a choice of either waiting for the package to get sorted out, or to do the build myself again. I decided to do my own build rather than live without xmonad, but rather than mucking about with tarballs I could at least now get the source from the package repository. The appropriate steps, for anyone interested or having the same problem, are: Make sure libxinerama-dev is installed Recompile libghc6-x11-dev and install it Recompile libghc6-xmonad-dev and libghc-xmonad-contrib-dev against the new X11 lib The apt-get incantations are: sudo apt-get install libxinerama-dev cd /tmp sudo apt-get source --compile libghc6-x11-dev sudo dpkg -i libghc6-x11-dev_1.4.1-1_i386.deb sudo apt-get build-dep libghc6-xmonad-dev sudo apt-get source --compile libghc6-xmonad-dev sudo dpkg -i libghc6-xmonad-dev sudo apt-get build-dep libghc6-xmonad-contrib-dev sudo apt-get source --compile libghc6-xmonad-contrib-dev sudo dpkg -i libghc6-xmonad-contrib-dev_0.6-4_i386.deb A quick alt-q restart, and all is well. I only mention all this because it's so easy in this day and age to take something like apt for granted, and every so often it's worth taking a moment to appreciate just how spectacularly good it really is. Where I work, deployments are an endless source of headaches and grief, yet the complexity of those deployments absolutely pales against the task of updating literally millions of systems, all slightly different to each other, thousands of times a day. It's just a joy to be able to say to apt \"hey, go get me everything I need to build package x, then build package x, then install it for me. And get it right first time!\". In most cases, it does just that. It's an astonishing piece of software.","tags":"commentary","title":"Ubuntu, Xmonad, and an Ode to Apt"},{"url":"dynamic-async-batching-with-pfx.html","text":"The PFX Team blog has been posting some excellent articles recently on the subject of task batching using the June 2008 CTP release of the Task Parallel Library. It's really cool to see some of these techniques abstracted properly in .Net, and I hope it eventually becomes part of the core libraries. I've been playing around a bit recently with the June CTP in the context of batching up web service calls, as that's something I do quite a lot. One particular problem that comes up occasionally is a two-stage series of requests to download a complete set of paged data. I might do this if I wanted to download an entire discussion thread, for instance, or a large account statement from my online bank. Typically in this situation the web service will limit the number of records I can retrieve in one request, and allow me to specify start and count parameters to the request. The response will also include a total record count, so I know how much data there is. The normal use case for this is to request the first page of data, and use the total record count to display a list of page links that my user can click on to navigate the data or jump to any page. In my case, however, I want ALL the data as quickly as possible. So, imagine a situation where I am using a service that lets me download a maximum of 200 records per request. My first step is to request the maximum 200 records starting from index 0, i.e. the first page of data. In the response will be a total record count - if that number is equal to the number of records I got back (i.e. <= 200) I've got everything in one hit and can stop. But what if the total record count is, say, 1000? I need to make four more requests (since I've already got records 1-200, I have 800 more to get in batches of 200 each). Naturally I want to do this asynchronously, using as few resources as I can. This means all webservice calls should be using the APM pattern (thus using IO completion ports, and not consuming worker threads from the thread pool or creating my own threads) and, preferably, not blocking anywhere except when I actually need some data before continuing. The two-stage process can be successfully captured asynchronously by combining a future and a continuation. I encapsulate the initial request in a Future object (which is a subclass of Task), and handle the check-record-count-and- get-more-records-if-required logic in the continuation. The code for this basically looks as follows: public Future < List < Item >> GetAllItemsAsync () { var f = Create < GetItemsResponse >( ac => Service . BeginGetItems ( 0 , ac , null ), Service . EndGetItems ); var start = 200 ; var resultFuture = f . ContinueWith ( r => { /* Batch retrieval here... */ }); return resultFuture ; } In order to support the APM pattern neatly, I'm using the following method from the PFX blog : private static Future < T > Create < T >( Action < AsyncCallback > beginFunc , Func < IAsyncResult , T > endFunc ) { var f = Future < T >. Create (); beginFunc ( iar => { try { f . Value = endFunc ( iar ); } catch ( Exception e ) { f . Exception = e ; } }); return f ; } This could be coded as an extension method, though I haven't bothered yet as I'm hopeful this immensely useful snippet will be integrated into the library itself. Now I need to make a number of calls to get the rest of the data, so I loop until I've made the required number of async service calls: var resultFuture = f . ContinueWith ( r => { var items = new ConcurrentQueue < Item >(); var handles = new List < WaitHandle >(); while ( start < r . Value . TotalRecordCount ) { var asyncResult = Service . BeginGetItems ( 200 , ar => Service . EndGetItems ( ar ). Items . ForEach ( items . Enqueue ), null ); handles . Add ( asyncResult . AsyncWaitHandle ); start += 200 ; } handles . ForEach ( h => h . WaitOne ()); return items . ToList (); }); I'm about 85% happy with this as an approach. I'm not completely happy, however, because of the WaitOne calls, which mean that I'm blocking on a threadpool thread until all the calls complete. Given that this is all wrapped up in a future, I may not actually need to access the data until well after the calls have completed, in which case I am wastefully consuming a threadpool thread for some period of time. So the $64,000 question is, how do I get rid of it? I'm sure there's a way to do it, but my brain has gone on a protest march about all the time I'm forcing it to spend thinking about this stuff.","tags":"coding","title":"Dynamic Async Batching with PFX"},{"url":"comment-discontent.html","text":"There seems to have been a recent outbreak in blog posts about code commenting . As is so often the case with topics such as this, everyone has an opinion and they all seem to be different. It's quite an eye- opener seeing some of the explanations, justifications, and outright haranguing used in defence of all sorts of weird and wonderful stances. I got a wry smile from stevey's post , as I recognise only too well the tendency to write narrative comments. I'm sure there's plenty of code from early in my career still floating around in various company codebases where the code/comment ratio is something embarrassing. I've mostly shaken that off now, though I sometimes have to fight my inner raconteur when writing something I think is neat or clever. Jeff Atwood, as is so often the case recently, contradicted his own previous post on the matter (replacing the statement \"comments can never be replaced by code alone\" with \"if your feel your code is too complex to understand without comments, your code is probably just bad\") and endearingly veered wildly to and fro across a sensible medium 1 , without ever quite hitting it. Coding Horror, indeed. So far, so blah; every time an argument on comments flares up we see the same thing. Something I've not noticed before though, either because I wasn't paying attention or because it's a new thing, is a trend amongst the I-don't -need-comments crowd to advocate very long and detailed method names as an alternative. As neophyte coders we all have it drilled into us that we must use descriptive names. Programming gospel, as handed down in sacred tomes such as Code Complete , tell us not to use names like 'i' and 'tmp' except in very specific circumstances (e.g. loop indexes and tempfile handles). And, without question, this is good solid advice. Take heed, young Padawan, etc. But can you take it too far? It's not something I've really come up against, but it seems to be increasingly popular. One response to Jeff's post suggested (only in passing, to be fair) using a function name like newtonRaphsonSquareRoot . A digg comment (OK, OK, not exactly the fount of all knowledge) vehemently defended the virtue of the frankly- scary RunEndOfMonthReportsUnlessTheMonthStartsOnAFridayInWhichCaseRunTheWeekl yReportInstead (!) The argument is that with names like these, you don't need comments, since it is perfectly clear what the function does. Is it perfectly clear at the wrong level though? Function names like this, in my opinion, are so 'clear' that they leak. These are function names that violate the principle of encapsulation . If I write a square root function, why do I need to burden all my clients with information about how I've implemented it? By naming it newtonRaphsonSquareRoot , that's exactly what I'm doing. Unless there are specific performance implications/requirements that favour Newton- Raphson , in most cases my clients just want a damn square root calculated to within a specified tolerance and don't care whether I used Newton's method or one of the army of alternatives . The implementation should be private to the method, and no-one else's business. Worse, what if a requirements change means a switch to Walsh's fast reciprocal method ? Uh-oh, now my method name is completely misleading, so I have to change it. Oops, now I have to change all the client code that calls it! I'd better hope no-one has exposed this with [WebMethodAttribute] since I wrote it, otherwise there could be thousands of client applications out there relying on it. My funky rename refactoring can't save me now. If every tiny change propagates through the system requiring hundreds of source files to change, and possibly external apps as well, you may as well just copy 'n' paste the code everywhere it's needed and doing away with the function completely. Hell, who needs abstraction anyway? We all do, of course, which is why I think names like this are a bad smell. The same goes for RunEndOfMonthReportsUnless... - what happens when the requirements change? This method name couples the public interface (method name) to the private implementation, which is exactly what you're not supposed to do. RunEndOfMonthReports is probably sufficient. Separate interface and implementation. This is programming 101, people, it shouldn't be beyond our grasp. The function name is descriptive and clear whilst remaining general enough to allow an alternative implementation. Anyone who cares enough about the implementation (for performance reasons, or simply curiosity) can find enough information in the comment to start their investigation, without having the details jammed in their face every time they call it. I agree with Dan Dyer that the best choice is as follows: /** * Approximate the square root of n, to within the specified * tolerance, using the Newton-Raphson method. */ private double approximateSquareRoot ( double n , double tolerance ) { double root = n / 2 ; while ( abs ( root - ( n / root )) > tolerance ) { root = 0.5 * ( root + ( n / root )); } return root ; } ↩","tags":"commentary","title":"Comment Discontent"},{"url":"lexical-closures-in-c-30.html","text":"There's a slightly weird article up on Dobbs Code Talk this week, speculating that aggregate functions are \"the next big programming language feature\" after closures. The slight weirdness comes from the fact that both features have been around for decades, and not just in dusty academic languages either. Still, there's some interesting discussion in the comments about whether .Net's closures are proper first-class lexically-scoped closures. The answer is yes - but with a fun twist. The twist has been around for a long time - Brad Abrams blogged about it way back in 2004 , for instance - but it's probably worth going over it again, since the recent arrival of LINQ and lambda syntax in C# 3.0 will presumably lead to more people being bitten by this as the use of closures becomes more mainstream. A key thing to remember is that C# lambdas are just anonymous delegates in skimpy syntax. Behind the scenes the compiler turns them into classes - if you were looking at disassembled MSIL you wouldn't be able to tell whether the code was written with lambda syntax or anonymous delegate syntax. Therefore, the issue discussed by Brad has not gone anywhere. Lets revisit the problem, with a 2008 sheen applied (i.e. I'll use lambda syntax rather than anonymous delegate syntax). What does the following code display? Func < int >[] funcs = new Func < int >[ 10 ]; for ( int i = 0 ; i < 10 ; ++ i ) { funcs [ i ] = () => i * i ; } funcs . ForEach ( f => Console . WriteLine ( f ())); If you answered something along the lines of \"prints the square of every number between 0 and 9\" you'd be...wrong. Really, try it out. See? Now, a lexical closure is supposed to capture its environment, meaning that the lambda stored on the first loop would capture i when i==0, the second loop would capture i when i==1, and so on. If this happened, then executing all the lambdas would indeed result in the squares of the numbers 0-9 being printed. So what gives? The problem stems from the fact that the lambda is binding itself to a variable that is accessible outside the closure, which is being changed in every iteration of the loop. The closure doesn't capture the value of i, it captures a reference to i itself, which is mutable. You could actually make a case that this is bad code anyway, since it gives two responsibilities to the loop index - control the loop, and act as data in the closures. If we were being pedantic, we could split the responsibilities by creating a new variable, j, to be the closure data each iteration, and let i concentrate on being an index: for ( int i = 0 ; i < 10 ; ++ i ) { int j = i ; funcs [ i ] = () => j * j ; } Lo and behold, the code now works! Pedantry rules! Take a look with Reflector or ildasm to see what's going on here. The executive summary is that the compiler captures the environment (i in the first example, j in the second) by creating a member variable within the class it generates for the closure. Previously, since the same instance of i lived for the entire duration of the loop, only one instance of the generated class was created and shared. Now, however, a new instance of the generated class is created in each iteration of the loop (since j is scoped within the loop body and thus we have a new j every time round). Thus, the data is not shared and we get the expected output. There are two important points to consider here: The problem goes away if you write code more declaratively. Do away with the clunky for loop and everything works OK. Enumerable . Range ( 0 , 10 ). Select ( x => x * x ); It isn't always bad that multiple closures can capture a reference - since one closure can 'see' updates made to the shared data by another closure, you could use this as a coordination mechanism. This is not an issue that's going to crop up every day - the example above is fairly contrived - but knowing about it will save some painful debugging sessions when inevitably you do run into it. The fix is always to take a local copy of the mutable data to coerce the compiler into generating code that creates multiple instances of the class generated to represent the closure. Simple, yes? ;-)","tags":"coding","title":"Lexical Closures in C# 3.0"},{"url":"project-euler-problem-5.html","text":"On to the next Project Euler problem (after a bit of a hiatus)... Problem 5 2520 is the smallest number that can be divided by each of the numbers from 1 to 10 without any remainder. What is the smallest number that is evenly divisible by all of the numbers from 1 to 20? In common with many of the other Euler problems, there's a brute-force way to solve this, and a clean algorithmic way. And in common with my other Euler posts so far, I'll start with the brute-force way ;-) This problem can be tackled head-on with the following approach: Start from n =1 and increment in a loop. Test each value of n by attempting to divide it by all numbers m from 1 to 20. The first number to pass the test (i.e. n mod m is 0 for all values of m ) is the answer. private static long BruteForceSolver () { long result ; for ( result = 1 ; ! Check ( result ); ++ result ) ; return result ; } private static bool Check ( long result ) { for ( int i = 1 ; i <= 20 ; ++ i ) { if ( result % i != 0 ) return false ; } return true ; } This works, but it takes >12 seconds to execute on my PC, so it's not what you'd call efficient (though it is well within the Euler execution time guidelines). Some speed gains can be achieved by exploiting the information provided in the question itself. We are told that 2520 is the lowest number evenly divisible by all numbers from 1 to 10. Since the problem space (1 to 20) includes all these numbers, the answer must also be evenly divisible by 2520. This allows much bigger increments each loop - rather than incrementing by 1, why not increment by 2520? And since the answer must be greater than or equal to 2520, why not start the loop there instead of 1? Finally, since we already know that 1 to 10 divide evenly into 2520, each inner loop only needs to check numbers 11 to 20. That should speed things up a bit: private long BruteForceSolver () { long result ; for ( result = 2520 ; ! Check ( result ); result += 2520 ) ; return result ; } private bool Check ( long result ) { for ( int i = 11 ; i <= 20 ; ++ i ) { if ( result % i != 0 ) return false ; } return true ; } And indeed, on my machine this is now down to 150ms or so. It's still not a very nice way to tackle the problem, though. Thinking about it from a different angle yields an altogether smarter approach. Imagine we are looking for the lowest number evenly divisible by the numbers 1 to 2. [1, 2] Well that's easy; since there are only two numbers we just find the lowest common multiple (LCM), which in this case is 2 (since 2 % 2 == 0, and 2 % 1 == 0). If we call this sequence s 1 , we can say that \\(LCM(s_1)=2\\) . OK, now imagine we are solving the same problem for s 2 , which contains the numbers 1 to 3. [1, 2, 3] You'll notice that s 2 contains s 1 in its entirety. \\(LCM(s_2)\\) must therefore be a multiple of \\(LCM(s_1)\\) , so we can rewrite s 2 as [ \\(LCM(s_1)\\) , 3], or [2, 3][2]=2 \\(). Now we are down to two numbers again, so we can calculate the LCM of 2 and 3, which is 6, so $LCM(s_2)=6\\) . OK, now we solve the problem for the first 4 numbers (s 3 ). [1, 2, 3, 4] This sequence contains s 2 , therefore \\(LCM(s_3)\\) is a multiple of \\(LCM(s_2)\\) . We can rewrite s 3 as [ \\(LCM(s_2)\\) , 4], or [6, 4]. Thus, \\(LCM(s_3)=12\\) . This can be repeated as many times as necessary. Generally, we have s n = [ \\(LCM(s_{n-1})\\) , n +1] where n > 0. This looks recursive, but a better way to think of it is as an excellent example of a fold. A fold is one of the fundamental tools of functional programming. In fact, it is perhaps the most fundamental, since map, filter etc can be implemented as right folds 1 . I won't inflict my pitiful Photoshop skills on anyone by trying to graphically represent a fold - try looking at this Wikipedia article if you want to try and visualise it. Broadly, the behaviour of a fold is to apply a combining function to elements in a list (or other data structure) and accumulate the results. That's exactly what we want here - our combining function is LCM, and our accumulating value is the LCM of the whole list. Effectively, for list s 3 above, we have $$LCM(s_3)=LCM(LCM(LCM(1,2),3),4)=12$$ Note how the result of the innermost LCM (applied to values 1 and 2) becomes a parameter to the next LCM, which in turn becomes a parameter to the outermost LCM which returns the result we want. By using a fold, we can generalise. In Haskell, the whole problem is a one-liner: foldl lcm 1 [ 1 .. 20 ] The 1 passed in as a parameter represents the terminating value to use when the end of the list is reached. It is common for this value to be the first element of the list, so Haskell provides a convenience function that removes the need to specify it as a parameter: foldl1 lcm [ 1 .. 20 ] Not all languages and platforms provide an LCM function right out of the box, so to take this neat Haskell solution and port it to .Net, the LCM function needs to be implemented. This is easily done in terms of the greatest common divisor (GCD) like so: $$LCM(a, b) = \\frac{a\\cdot b}{GCD(a, b)}$$ .Net doesn't provide a GCD function either, so I'll implement it using Euclid's Algorithm as an extension method on long ints: public static long GCD ( this long a , long b ) { while ( b != 0 ) { long tmp = b ; b = a % b ; a = tmp ; } return a ; } With GCD defined, LCM can be implemented as above: public static long LCM ( this long a , long b ) { return ( a * b ) / a . GCD ( b ); } With this in place, it's a simple matter to use .Net's equivalent of fold - a method on IEnumerable<T> called Aggregate - to get the answer 2 : return LongEnumerable . Range ( 1 , 20 ) . Aggregate ( 1L , ( curr , next ) => curr . LCM ( next )); And indeed, the same basic pattern can be used to solve the problem in a number of languages. In F#, given implementations of LCM and GCD as above, we have: List . fold_left lcm 1 [ 1 .. 20 ] And in ruby: require 'rational' ( 1 .. 20 ) . inject { | c , n | c . lcm n } Given that the right algorithm makes this problem a fairly trivial expression in all these languages, it's pretty hard to identify which is the nicest. I think overall I'll give the nod to Haskell, however, for not making me implement LCM and because I find ruby's 'inject' a less intuitive function name than foldr (but that's probably because I learned the technique in Haskell in the first place and am set in my ways...) [2]: since we know $LCM(s_1 For example, in F#: let filter p lst = List . fold_right ( fun x xs -> if p x then x :: xs else xs ) lst [] let map f lst = List . fold_right ( fun x xs -> f x :: xs ) lst [] ↩ Note that in this code LongEnumerable is just a very simple partial reimplementation of Enumerable, using longs instead of ints ↩ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"projecteuler","title":"Project Euler Problem 5"},{"url":"project-euler-problem-4-extra.html","text":"Couple of things to add to yesterday's post about problem 4 . As is so often the case in life, no sooner had I finished the article than I realised there was an obvious additional step I could make, which I'd somehow failed to spot. Regarding the C# solution, an easy win having implemented the Reverse extension method would be to add an IsPalindrome extension to the string class too. The implementation is straightforward: public static bool IsPalindrome ( this string s ) { return s == s . Reverse (); } With this done, the where clause in the LINQ query is more readable, and we have a couple of handy reusable string extensions into the bargain. var result = ( from product in AllProducts . From ( 100 ). To ( 999 ) where product . ToString (). IsPalindrome () select product ). Max (); Also, Sol commented that the C code could have a direct implementation of a palindrome function, rather than messing about with strrev, since the implementations are very similar. Whilst this series isn't really focussed on the performance benefit of this approach, it does also make the code more expressive, so I'll include it: int strpalindrome ( char * s ) { char * s1 , * s2 ; s1 = s2 = s ; while ( * s2 ) s2 ++ ; while ( s1 < s2 ) { if ( * ( s1 ++ ) != * ( -- s2 )) return 0 ; } return 1 ; } The loop now looks like this: for ( i = 100 ; i < 1000 ; ++ i ) { for ( j = i ; j < 1000 ; ++ j ) { int sum = i * j ; sprintf ( s1 , \"%d\" , sum ); if ( strpalindrome ( s1 ) && sum > largest ) { largest = sum ; } } } If you are interested in the Project Euler problems but craving more detailed analysis, Joel Neely is working through at a similar rate to me, but focusing his efforts on Scala and studying each problem and its solution in greater depth rather than flitting from language to language. Highly recommended.","tags":"projecteuler","title":"Project Euler Problem 4: Extra"},{"url":"project-euler-problem-4.html","text":"Problem 4 is as follows: A palindromic number reads the same both ways. The largest palindrome made from the product of two 2-digit numbers is 9009 = 91 × 99. Find the largest palindrome made from the product of two 3-digit numbers. Bit of an easy one, this. The approach is pretty simple to understand - first calculate all the products of every pair of numbers between 100 and 999, then filter for the palindromic ones, and finally select the largest. The only even vaguely tricky bit is determining if the number is palindromic. The easiest check is to simply convert the number to a string, and check if the string is equal to itself when reversed. To shake myself out of C#/python complacency, I decided to write my first attempt at this in good old C. I'm a bit rusty so this took a few goes to get right (the shame). First, I need a string reverse function. For those of us that learned to program when C and C++ were king (before that upstart Java came long and ousted languages with pointers from classrooms up and down the land), this is bread and butter. char * strrev ( char * s ) { char * s1 , * s2 ; char c ; s1 = s2 = s ; while ( * s2 ) s2 ++ ; while ( s1 < s2 ) { c = * ( -- s2 ); * s2 = * s1 ; * s1 ++ = c ; } return s ; } If you can't read that, shame on you, go and pick up a copy of K&R and read it until you weep. In the meantime, basically what happens here is I set pointers to the start (s1) and end (s2) of the original string (s), then swap the pointed-to characters using a temporary variable (c) and move both pointers 1 character towards each other. Repeat until they meet in the middle. In this day and age of immutable strings, this old friend now feels a little weird - although I retain (and eventually return) the original pointer, I have in fact modified the actual string that was passed in. Contrast this with C's trendy modern progeny, where you can't change a string at all and have to use a StringB[uilder|uffer] when mutating strings (unless lousy performance makes you smile, of course). Still, now it is fairly simple to solve the problem. A nested for loop will let me calculate all the products, and then I just need to convert the results to strings and do the palindrome test. I keep track of the largest palindromic number found so far, and print it at the end. int main () { int i , j ; int largest = - 1 ; char s1 [ 7 ]; char s2 [ 7 ]; for ( i = 100 ; i < 1000 ; ++ i ) { for ( j = i ; j < 1000 ; ++ j ) { int sum = i * j ; sprintf ( s1 , \"%d\" , sum ); strcpy ( s2 , s1 ); strrev ( s2 ); if ( strcmp ( s1 , s2 ) == 0 && sum > largest ) { largest = sum ; } } } printf ( \"%d \\n \" , largest ); return 0 ; } Note I'm using the much-maligned strcpy function here (the cause of most of the buffer overflow attacks that were so endemic a few years back), but since I completely control the input it's no problem. Also note the use of sprintf to convert the int to a string, and I have to make a copy of the resulting string since my strrev function is destructive. The char arrays are of size 7, since the largest possible product in the problem space is 999*999=998001 which is 6 digits - plus 1 for the null terminator. Which I didn't forget about at all in my first attempt at this, nosirree. To make the code a bit more 'modern' I could do the allocation and copy in the strrev function, so that the passed-in string remains unchanged and a new string gets returned, but without a garbage collector to rely on this potentially leads to memory leaks (since strrev allocates the memory but relies on the caller to free it - easy to forget!). Anyway, I'm wallowing in nostalgia here so who cares about modern idioms. Whilst I chose C for this on a whim, it is (as always) enlightening to write a little C now and then, as it reminds you of the cost of things that are often taken for granted. Some people still don't understand why a StringBuilder offers better performance (trust me, I have interviewed more than a few of them), and are happy to write string manipulation code using immutable strings that results in countless allocations and destructions taking place, for no justifiable reason. Writing some string manipulation (or anything else) in C is a nice way to regain a bit of insight and perspective if you are spoiled by quad-core PCs and high-falutin' generational garbage collectors and smartass runtimes that don't let you write past the end of an array. So, now we've got a nuts 'n' bolts reference implementation, let's look at some more exotic approaches. As regular readers will have noticed by now, I'm fairly keen on LINQ - so it should be no surprise that C# is my next port of call. I was amused to recall that the .Net string class lacks a Reverse method so I had to write my own for this, about 20 minutes after I finished pontificating about the clean healthy virtue of doing so in C! (I didn't plan it this way, honest.) There are of course many ways of writing a string reversal routine, but rather than attempt to mimic the fairly idiomatic C code above, I did the simple thing: public static string Reverse ( this string s ) { char [] ch = s . ToCharArray (); Array . Reverse ( ch ); return new string ( ch ); } Since the Array class contains a Reverse method, I can just convert my string to an array (of chars), reverse that, then create a new string from it. Done. Things to meditate on regarding this approach: It relies on a char array. Strings may look like a native type these days, but I have to expose their dark and shameful lineage to get the job done here. The throwback nature of this implementation does not, of course, extend as far as modifying the parameter. A new string is returned and the original remains intact. The garbage collector will take care of deallocation. This is an extension method on System.String, so I can use it naturally on any string. Whatever faults it may have, it's definitely easier to read than the C code, since the syntax is much closer to the problem domain. This is a recurring theme when looking at expressiveness. The C code has to specify the entire algorithm for reversing the string, whereas here the Array.Reverse method allows us to ignore the details of how the string is reversed. For the purposes of this problem, we don't really care how the string is reversed, just that it is reversed. It's still warty, however, in that we have to know to turn the string into an array first, which may be completely non-intuitive to someone who's never tangled with C-style strings. With this minor omission from the .Net libraries sorted, the problem can be solved with a single compound LINQ query: return ( from product in ( from i in Enumerable . Range ( 100 , 900 ) from j in Enumerable . Range ( i , 1000 - i ) select i * j ) where product . ToString () == product . ToString (). Reverse () select product ) . Max (); I think that's pretty concise, and quite readable too. The main thing I don't like about it is the use of Enumerable.Range, which takes 'start' and 'count' parameters rather than 'from' and 'to', which would look more natural in this case. Parameters aside, it's interesting to note the relative clumsiness of the twin calls to Enumerable.Range. Back when looking at problem 1 , replacing a for loop with a more declarative alternative made the code considerably more expressive. In this case, however, I don't think it helps quite so much. Once again, it's to do with the nature of the problem domain - a nested for loop is quite a natural way to represent the process of generating the products, so the benefit of a declarative approach is less marked. How to improve it? For fun, lets go the whole hog and make a simple fluent interface to improve readability of the LINQ query. This is what we want: return ( from product in AllProducts . From ( 100 ). To ( 999 ) where product . ToString () == product . ToString (). Reverse () select product ). Max (); Nifty, huh? Well actually I have my reservations about fluent interfaces, but it's quite the fashion these days so I thought I'd give it a chance. The example above is trivial to achieve. On a class called AllProducts we need a static method called From which acts as a factory method, and an instance method called To which returns an IEnumerable<int> for use in the LINQ query. The class looks like this: class AllProducts { private int m_from ; private AllProducts ( int @from ) { m_from = @from ; } public static AllProducts From ( int from ) { return new AllProducts ( @from ); } public IEnumerable < int > To ( int to ) { int inclusiveTo = to + 1 ; // to is inclusive return from i in Enumerable . Range ( m_from , inclusiveTo - m_from ) from j in Enumerable . Range ( i , inclusiveTo - i ) select i * j ; } } Fluent interfaces are definitely this season's black. I heard about a guy who wrote a mocking library without using fluent interfaces for expectations, and 500 angry TDD advocates chased him out of the building with pitchforks. I'm still a bit wary though, tweedy programmer as I am - I just get a bit nervous about writing hideously contorted classes with a mixture of static and instance methods, some returning the this ref, some returning arbitrary IEnumerables, some acting as factories - and all so the calling code can prance about in a tailored coat and a cool pair of shades. A noble goal, to be sure, but is the price too high? I guess we'll know in a year or two when some maintenance programmer has to try and debug it. Enough of the lousy clothes metaphor. To finish up what turned out to be a longer post than expected, here's an F# solution I hacked together before getting sidetracked with the whole fluent thing. I read a blog post here about problem 4 ( warning - also contains solution to problem 6) but didn't like it too much. It seems to be quite common when reading F# code on the web for there to be a reliance on Seq.unfold - I'm not sure it's always the right tool. Then again, my F# is sketchy at best for the moment, so maybe I should shut up. For balance, however, this is my solution without using unfold. Note first that F#, as a .Net language, also lacks a built-in way to reverse strings. The implementation is extremely similar to the C# approach above: let rev ( s : string ) = let ch = s . ToCharArray () let ra = Array . rev ch let r = new string ( ra ) r The actual implementation involves two helpers: a small recursive function to produce all possible products of the numbers contained in two lists, and a simple check to see if a string is equal to itself reversed. let Euler4 = let rec allProducts l1 l2 = let mul a lst = List . map ( fun x -> a * x ) lst match l1 with | [] -> [] | h :: t -> ( mul h l2 ) @ ( allProducts t l2 ) let isPalindrome x = let str = Int32 . to_string x str = rev str With the plumbing in place, we can use the very groovy forward pipe operator to chain together some very readable code. The only thing to note in here is the reverse ordering of the parameters in the call to compare - this is because we want the results in descending order, so that the largest is at the head of the list and easily accessible with List.hd. allProducts [ 100 .. 999 ][ 5 ] |> List . filter isPalindrome |> List . sort ( fun x y = compare y x ) |> List . hd","tags":"projecteuler","title":"Project Euler Problem 4"},{"url":"test-specific-shims-in-production-code.html","text":"We're currently on a fairly major kick to increase automated test coverage of our software. This doesn't just mean 'get the unit test coverage up to scratch', it also means we are working towards full end-to-end integration testing using, amongst other tools, some front-end automation tools such as QTP and Selenium . Of course, nothing is ever easy when trying to polish away the tarnish of ancient code. One particular problem we face regularly is patching up code that breaks the fragile expectations of some of these automation tools. Some of our applications - including the one I am working to refactor - contain UI widgets that use a lot of custom painting routines and conceal data pretty well. One widget, for instance, needs to display data with a fast refresh rate and so uses a double-buffered approach to avoid flicker. The data it displays, however, is not stored anywhere; it is discarded as soon as it is rendered. And since the whole widget view is rendered as a bitmap and blitted to screen, there's no convenient hierarchy of panels, labels, text boxes, or any other standard controls. This, the Automated QA folks tell me, causes a problem since QTP mainly works by reflecting on properties exposed by controls to get at their data. So, if QTP wants to read some data from a text box, it accesses the Text property of that text box. Simple. But this particular widget doesn't have the equivalent of a Text property. This isn't really an oversight from a purely functional point of view, since no part of the actual application code ever needs to get data from the widget - it's a display mechanism only, not an interactive widget like a text box. Data is received from a web service, processed a bit, and dumped into the widget. The widget is the last object to do anything with the data - no other part of the app ever needs it again. Since there are no properties on the widget exposing the data, QTP can't get at it. Of course, there are ways to keep QTP happy. We can add a few properties to the widget and keep some data around in member variables, or we can write some extensions for QTP that allow it to access some of the widget's internals. The second way is probably the 'right' way since it keeps test-related code external to the application code - but it's more time-consuming, and also has a training cost since most developers aren't going to be familiar with QTP's API. This leaves the first option. Traditionally I've always been a bit wary of having what is effectively test code (since it only exists for testing purposes) deployed with production code. Furthermore, doesn't it undermine the tests themselves, since they are dependent on code that never gets executed in production? On the other hand, in some instances it may be the more pragmatic thing to do. It's difficult to justify spending a day or two writing a few hundred lines of QTP extension code when the same effect can be garnered by adding a single read-only property. It still doesn't quite sit right for me though, and I can't find much in the way of authoritative literature that argues one way or the other.","tags":"development","title":"Test-specific Shims in Production Code"},{"url":"bash-history-spelunking.html","text":"Learned from Weiqi, who learned from KageSenshi , about a Fedora Planet shell history meme - post the results of running the following command on your linux box: history | awk '{a[$2]++ } END{for(i in a){print a[i] \" \" i}}' | sort -rn | head I won't bother repeating the inevitable warning about the dangers of executing random shell scripts you find on the Internet, because I'm lazy and mean. Anyway, here's the results from my webhosting box: 231 ll 171 vim 132 cd 50 screen 43 cat 39 tail 34 ls 34 cls 32 exit 31 wget 'll' is an alias for 'ls -l', and 'cls' an alias for 'clear'. No real surprises otherwise - I use vim for development over ssh, I tail my logs occasionally, and live in GNU Screen. Here's the output from my home box: 254 ll 181 cd 148 sudo 123 rm 123 ffmpeg 86 screen 83 ls 75 cls 72 vim 60 find Quite similar actually, guess I'm set in my ways. The ffmpeg count is a bit of an anomaly, since I used it a lot recently to re-encode a bunch of Futurama rips for my mobile. Not sure what to do with this remarkable intel, however. Perhaps I'll use the data to generate an Identicon and use it as a favicon? Or, perhaps not.","tags":"tools","title":"Bash History Spelunking"},{"url":"evil-important-apparently.html","text":"OK, I know I shouldn't even acknowledge spam blogs, but this one amused me. Some filthy credit-crunch link bait site took an extract from my previous post (this is obviously what happens when you say the phrase 'credit card' ... oops) and ran it though an automated word substitution program. The result is fascinating. It turned this: … you buy something from Amazon, you are protected by the fact that evil black-hats can't find the prime factors of your encryption key fast enough to steal your credit card number (OK, bit of a generalisation, but that's the gist). … into this: … you take something from Amazon, you are secure by the fact that important black-hats can't connexion the matureness factors of your writing key alacritous adequacy to advise your assign calculate sort (OK, discernment of a generalisation, but that's the gist). … Let's review the highlights. 'Buy' replaced with 'take', 'evil' replaced with 'important', 'steal' replaced with 'advise'? Someone's book of synonyms is bound in human hide with a skull on the front.","tags":"commentary","title":"Evil = Important. Apparently."},{"url":"project-euler-problem-3.html","text":"Next up in the list of Project Euler problems is this one: Problem 3 The prime factors of 13195 are 5, 7, 13 and 29. What is the largest prime factor of the number 600851475143? This, obviously, is a factorisation problem. There is a colossal amount of material on the web for dealing with prime factorisation - a simple google search pulls up lots of information. Prime factorisation (and the difficulty of doing it with sufficiently large numbers) is at the heart of the cryptographic methods we currently use on the internet - every time you buy something from Amazon, you are protected by the fact that evil black-hats can't find the prime factors of your encryption key fast enough to steal your credit card number (OK, bit of a generalisation, but that's the gist). One of the key phrases in the above paragraph is 'sufficiently large numbers'. For a computer, 600851475143 is not a particularly big number, so this problem can be brute-forced fairly easily. Of course, not all brute force approaches are created equal. The most naive algorithm would be something along the lines of a three-pass sweep - firstly test every single number between 2 and 600851475143 to see if it divides cleanly into 600851475143 (pass 1); then test each factor from pass 1 to see if it is prime (pass 2); and finally take the biggest of the pass 2 numbers to get your answer (pass 3). This would work, but it sucks. Fortunately, it's easy to optimise. Let the prime factors of our number N be f 1 , f 2 ... f n . If I start with the lowest prime number and work up from there looking for a factor, I know that the first factor I find will be prime (since if it wasn't prime, it would have factors of its own, which by definition would also be factors of our target number). This number is f 1 . I can divide the target number by f 1 and then factorise the result to find f 2 . Continuing this process will result in a list of prime factors, and then it's simply a case of selecting the largest. I can optimise further by not resetting the factor to the lowest prime number each time - since having found f 1 I know that there aren't any smaller factors, so I don't have to waste time looking for them. Here's the implementation in python: def primeFactors ( n , factor ): factors = [] while ( n % factor != 0 ): factor = factor + 1 factors . append ( factor ) if n > factor : factors . extend ( primeFactors ( n / factor , factor )) return factors print max ( primeFactors ( 600851475143 , 2 )) Note that in the recursive call the current factor is retained, so that the code doesn't repeat itself. This executes pretty quickly, but it could be better. For a start, since 600851475143 is odd there's no need to start with the only even prime number (2). Instead, I could just start at 3, and in the while loop skip over even numbers. This would cut the number of tested numbers in half. A more efficient trial division approach, however, would be to generate a list of primes, divide 600851475143 by each prime to find the prime factors, then simply select the largest. To use this solution, a prime number generator is needed. This is an interesting diversion - I've peeked at some of the other Project Euler problems and know that prime numbers will pop up again, so it may prove useful to have a generator handy for when I get to those. Some languages, like Ruby, have library functions that can give you primes, but other languages don't. If you're not interested in generating primes and just want to know the answer to problem 3, execute the code above and you're free to get down from the table. A Random Walk Off-Topic The simplest way to generate primes is known as the Sieve of Eratosthenes after the Greek mathematician who invented it. In principle it's straightforward - take a list of all integers up to an arbitrary limit, then starting from 2 (the smallest prime), mark all the numbers that are multiples of 2. Then move to the next unmarked number (i.e. 3) and mark all the multiples of 3. Then you move to the next unmarked number (5, since 4 was marked as a multiple of 2) and mark all multiples. And so on, until you get to the end of your list. Whatever numbers remain unmarked are all the primes up to your arbitrary limit. .Net lacks a built-in prime generator, so to demonstrate the algorithm I'll create a simple C# implementation. The list of numbers is represented as an array of booleans, all set to true by default except indexes 0 and 1 (since we aren't interested in evaluating those numbers as prime). The other requirement for a funky contemporary .Net implementation is, of course, to expose the results with IEnumerable. This achieves two things - firstly, it lets the sieve class control enumeration and thus skip over the marked numbers (making the calling code cleaner), and secondly it lets me use LINQ to query it. So, here's the code: public class SieveOfEratosthenes { private bool [] m_numbers ; public SieveOfEratosthenes ( long limit ) { m_numbers = new bool [ limit + 1 ]; for ( long l = 2 ; l < m_numbers . LongLength ; ++ l ) { m_numbers [ l ] = true ; } for ( int i = 2 ; i != - 1 ; i = Array . FindIndex ( m_numbers , i + 1 , b => b == true )) { for ( int j = i * 2 ; j < m_numbers . Length ; j += i ) m_numbers [ j ] = false ; } } public IEnumerable < long > Primes () { for ( long i = 2 ; i < m_numbers . LongLength ; ++ i ) if ( m_numbers [ i ]) yield return i ; } } Fairly straightforward. Basically, I start by marking 2 as prime. Then, an inner loop sets all multiples of 2 to false, since no (other) even numbers are prime. Each time round the loop, we find the next true element of the array (which will have a prime index), and mark all multiples false as per the description above. The loop terminates when FindIndex fails to find any more true elements. This results in an array where the only elements with a value of true are those with a prime index. This makes the actual IEnumerable generator very easy to write - it yield returns the index whenever it finds a true element. There's a problem with this code, however, that makes it unusable with Euler problem 3 (at least in .Net - hence why I called it a 'diversion' earlier, rather than an alternative solution). In .Net, you can't create an array with 600851475143 elements, since an array with 600851475143 elements is way above the maximum array size limit of 2GB. Even if each element is only a single byte, 600851475143 bytes is about 560GB. Therefore, you can't create a sieve big enough to solve the problem. When using trial division it seems that it is enough to only generate primes up to the square root of N, though there are cases when this is not true (e.g. where N=15, sqrt(N) = \\~3.873, but the largest prime factor of 15 is 5), and I don't have the maths (yet!) to know how big a fudge-factor is needed. I've seen a solution on the Project Euler forum that generates primes up to sqrt(N) + 10, which solves the example of N=15 above, but does it solve ALL cases? Another approach might be to generate the list of primes such that the largest prime in the list is the first prime > sqrt(N) - but now I'm completely guessing. Still, for numbers that fit inside the 2GB limit, we can find the largest factor easily. var sieve = new SieveOfEratosthenes ( 15 ); Now I have my IEnumerable sieve, I can craft a LINQ query to find the largest factor. All I need to do is filter my list of primes for those which divide directly into 15 and call the Max() method: return ( from p in sieve . Primes () where 15 % p == 0 select p ). Max (); Done! And I have a handy reusable prime generator for later on.","tags":"projecteuler","title":"Project Euler Problem 3"},{"url":"lightbulb-moment.html","text":"Weiqi Gao has a post up today discussing the trials of grokking Scala . Scala is a language I want to take a much closer look at later this year, since I want to become current on the JVM again (having not been on talking terms with it since using J2SE 1.4 around the summer of 2003) without being particularly keen on tangling with the Java language itself. One of the key features of Scala is the functional programming style it brings to the JVM. It's actually quite common to use certain functional idioms in Java - e.g. passing around a function as a parameter - but the syntax is clunky and verbose (unless and until closures get confirmed in 1.7, that is, and maybe even then). For example, take a look at this very simple idiomatic code for spinning off a thread to perform an expensive operation: new Thread ( new Runnable () { public void run () { someObj . doExpensiveOperation (); } }). start (); Now that's not the most hideous code I've ever seen, but it's a bit...wordy. Compare it to this equivalent implementation in Java's closest mainstream relative, C#: new Thread ( x => someObj . DoExpensiveOperation ()). Start (); I much prefer this syntax, even taking into account the throwaway lambda parameter that's only there to satisfy the ThreadStart signature. The Scala syntax is even nicer, however: spawn ({ someObj . doSomethingExpensive }) This is the sort of thing that piques my interest about the language - expressive syntax and a very funky concurrency model will get my attention, especially when running on something as mainstream as the JVM and with full interoperability with the frankly staggeringly-vast Java library ecosystem. I like F# on the CLR for similar reasons. But I digress; what I wanted to talk about was a point made by Weiqi, when discussing the pattern-matching capabilities of Scala: Pattern matching in Scala is exactly the point at which I would spend time trying to understand it, trying to master it, trying to learn to use it. I understand the syntax. I understand the explanation that the speakers in presentations gave. I do get to the part where I say \"This is cool.\" But I never get to the point where I would see a problem and say \"This problem is best solved with pattern matching, let me fire up Scala and code the solution.\" This strikes a chord for me, as I have gone through that stage once or twice myself with other features in other languages and yet can't quite put my finger on how I get past it. I don't think it's something you consciously do - it's just something you keep grafting away at until suddenly you realise that the technique, whatever it is, has become part of your armoury. Closures are an obvious example I can think of in my own background. I was raised as a straight-down-the-middle C++ man, way back in the early/mid-90s, cutting my teeth on Borland Turbo C++ 3 on Windows 3.1. When I first started to play with functional languages it took a long time for me to 'get it', and even when I understood what a closure was after a couple of weekends hacking around in OCaml, I couldn't envisage when I'd ever need one. Soon after, whilst working on a Konfabulator widget in javascript, I noticed I was using them all the time. I suddenly had much more insight into what ruby blocks were doing. It wasn't so much that I noticed the lights go on - they'd been on for some time and I hadn't realised. People commonly refer to the 'lightbulb moment' or 'the lights went on' as being the point where a flash of inspiration hits and everything suddenly makes sense. I don't like this metaphor. If I need to go to the bathroom in the middle of the night, when the lights go on I squint in pain and stagger around just as blindly as I did before. But then I acclimatise, and all becomes clear. And so it is, I think, with learning alien concepts - you need a bit of time to adjust to the dazzling light.","tags":"commentary","title":"The Lightbulb Moment"},{"url":"project-euler-problems-1-and-2.html","text":"Browsing through Nate Hoellein's blog recently led me to Project Euler . This is a problem - I have a horrendous feeling I'm about to get addicted to it, to the cost of just about everything else that normally occupies my free time. Ack. Still, at least it provides some blogging material. I'm going to start working my way through the list, and try to create idiomatic solutions in a number of languages. I won't always look for the most efficient solution, since I'm also interested in expressiveness (see here and here for previous posts on the subject). To start with, here's some code and thoughts for problems 1 and 2. Problem 1 Add all the natural numbers below 1000 that are multiples of 3 or 5. This is generally regarded as the easiest Euler problem, so shouldn't present too many problems. Mainstream software development is still dominated by imperative languages and styles, so the most recognisable solution to this would be a straightforward for-loop. Here is an imperative C# solution: int result = 0 ; for ( int i = 0 ; i < 1000 ; ++ i ) { if ( i % 3 == 0 || i % 5 == 0 ) result += i ; } Simple enough, but as with all for-loops the guts are a little too visible. I have to explicitly declare and increment an accumulator variable as well as the loop counter. A functional style (Haskell in this case) allows a more declarative solution: sum $ filter ( \\ n -> n ` mod ` 3 == 0 || n ` mod ` 5 == 0 ) $ [ 1 .. 999 ] Or, with list comprehensions: sum [ n | n <- [ 1 .. 999 ], n ` mod ` 3 == 0 || n ` mod ` 5 == 0 ] In both cases, the loop is replaced by a list generated from Haskell's range operator. [1..999] creates a list containing every integer between 1 and 999 inclusive. The modulo test is basically the same, though Haskell lacks a modulo operator (% in most C-family languages) so the mod function is used instead. Just for fun, here's an F# solution too: let sum = List . fold_left (+) 0l et mod35 = fun x -> x % 3 = 0 || x % 5 = 0L ist . filter mod35 [ 1 .. 999 ] |> sum This could be compressed into a one-liner like the Haskell solutions, but it would be a bit long for my taste. Also note F# is slightly hamstrung by the lack of a built-in sum function, so I have to define my own using fold. Another F# solution is here , but I prefer mine. There's a very nice snippet in the comments of that page, though, which I like even more: Seq . fold1 (+) { for i in 1 .. 999 when i % 3 * i % 5 = 0 -> i } Interestingly, C# is gaining some fairly powerful functional techniques lately, in particular LINQ. I can use the new Enumerable class to mimic range syntax and filter functionality from other languages, and lambdas to keep the code concise. Enumerable . Range ( 1 , 999 ). Where ( f => f % 3 == 0 || f % 5 == 0 ). Sum (); Note the similarity between F#/Haskell's lambda syntax and that of C#. It's very cool that a mainstream C-derivative language is getting this sort of syntax added to it. Alternatively, I could use LINQ query expressions for a different approach: var nums = from n in Enumerable . Range ( 1 , 999 ) where n % 3 == 0 || n % 5 == 0 select n ; nums . Sum (); Fun! It should be noted that all the Project Euler problems I've seen so far have mathematical solutions, meaning if you are able to classify the problem correctly it is straightforward to work out the answer with pen and paper. In this case, the problem is based around an arithmetic progression, and there are powerful formulae for reasoning about those. If you're interested, check out the forum for problem 1. Problem 2 Each new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... Find the sum of all the even-valued terms in the sequence which do not exceed four million. Ooh, Fibonacci. I've been here before . Using the Haskell code from that post makes problem 2 a snip: fib = 0 : 1 : zipWith ( + ) fib ( tail fib ) sum $ filter even $ takeWhile ( < 4000000 ) fib Given the lazy Fibonacci generator in the first line, this just uses standard Haskell functions from the Prelude to do all the work - reading right to left, takeWhile pulls data from the fib sequence until the test fails (i.e. we've reached 4,000,000), filter even does exactly what it says on the tin, and sum does the business on the result. C# could solve problem 1 very neatly - can it keep up the pace in problem 2? Actually, yes it can, after a fashion. The lazy Fibonacci generator can be implemented using the yield statement added in C# 2.0. This is much more efficient than the naive recursive solution I looked at in my previous post about Fibonacci sequences. Once I have the generator, the LINQ statement is very concise and quite similar to the Haskell code - C# 3.0 even has TakeWhile! IEnumerable < long > Fibs () { long a = 0 , b = 1 ; while ( true ) { yield return b ; b = a + b ; a = b - a ; } } Fibs (). TakeWhile ( f => f < 4000000 ). Where ( f => f % 2 == 0 ). Sum (); The more I use C# 3.0, the more I like it - there's quite a bit of power in there. As with problem 1, there are some fascinating mathematical tricks that can be utilised when solving problem 2, and I recommend you check out the forum . It's particularly cool to see how the Golden Ratio can be brought into play when working with Fibonacci sequences - I had no idea these techniques existed. So much to learn!","tags":"projecteuler","title":"Project Euler Problems 1 and 2"},{"url":"pg-wodehouse-method-of-refactoring.html","text":"I am much given to ruminating on refactoring at the moment, as one of my current projects is a major overhaul of a fairly large (>31,000 lines) application which has exactly the kind of dotted history any experienced developer has learned to fear - written by many different people, including short-term contractors, at a time in the company's life when first-mover advantage was significantly more important than coding best-practice, and without any consistent steer on the subjects of structure, coding conventions, unit tests, and so on. In other words, here be dragons. In fairness, the application works and has been a critical part of a company that has gone from nothing to market-leading multinational in 7 years, so it has certainly pulled its weight. It is in desperate need of a spring-clean though, and my team volunteered to spend 3 months evicting the cobwebs and polishing the brasswork. Yes, volunteered - it's a fascinating challenge, though perhaps not something you'd want to make a career of. Now, the first mistake to avoid here is the compulsion to throw it away and rewrite from scratch. So often when confronted with a vast seething moiling spiritless mass of code a developer throws his hands into the air and declares it a lost cause. How seductive is the thought that 31,000 lines of code could be thrown away and replaced with ~15,000 lines of clean, well-designed, beautiful code ? Sadly, that's often a path to disaster. It's almost a rule of the game. jwz left Netscape because he knew their decision to rewrite from scratch was doomed. Joel Spolsky wrote a rant about the same decision - in fact, the Netscape rewrite is commonly cited as a major factor in Netscape losing the first browser war. The problem is that warty old code isn't always just warty - it's battle- scarred . It has years of tweaks and bug-fixes in there to deal with all sorts of edge conditions and obscure environments. Throw that out and replace it with pristine new code, and you'll often find that a load of very old issues suddenly come back to haunt you. So, a total rewrite is out. This means working with the old code, and finding ways to wrestle it into shape. Naturally, Working Effectively With Legacy Code now has an even more firmly established place on my 'critical books' bookshelf than it did before. Inspiration came from a less well-known book, however. Buried in Chapter 10 of Code Reading is a single paragraph suggesting that it can be useful when working with unfamiliar code to paste it into a word processor and zoom out, getting a 'bird's eye' view. One other interesting way to look at a whole lot of source code quickly under Windows is to load it into Microsoft Word and then set the zoom factor to 10%. Each page of code will appear at about the size of a postage stamp, and you can get a surprising amount of information about the code's structure from the shape of the lines. (Spinellis, 2003) The idea is that this lets you immediately identify potential trouble spots - if you see pages where the code is all bunched up on the right, it indicates massive nesting and over-long functions. If you see heavy congestion, it indicates dense code. It's also easy to spot giant switch statements and other crimes against humanity. Of course, you don't actually need MS Word to do this - the Print Preview in Open Office is more than sufficient, and no doubt most office suites can do the same. This 50,000ft view could be a useful tool in tracking progress. I mean sure, we can have our build system spit out cyclomatic complexity and code size metrics, but wouldn't it be neat if we could do a weekly bird's-eye printout of the source code and pin it up on the wall, giving a nice simple visual representation of the simplification of the code? Except, of course, that with average page lengths of 45 lines we'd need almost 700 pages each time, and a hell of a lot of wall space. A better solution would be to print a class per page. At the start of the project, the application had about 150 classes, and the refactoring effort is focussed on about 80 of those. Initially, gigantic classes would be an incomprehensible smudge of grey, but as the refactoring process starts tidying the code and factoring out into other classes, the weekly printout would start to literally come into focus , hopefully ending up with many pages actually containing readable code (which happens roughly when the class is small enough to fit on no more than 3 pages at normal size). The first time we pinned up the printouts, I suddenly recalled a Douglas Adams foreword reprinted in The Salmon of Doubt . Adams was a great fan of P.G. Wodehouse, and explained Wodehouse's interesting drafting technique: It is the next stage of writing—the relentless revising, refining, and polishing—that turned his works into the marvels of language we know and love. When he was writing a book, he used to pin the pages in undulating waves around the wall of his workroom. Pages he felt were working well would be pinned up high, and those that still needed work would be lower down the wall. His aim was to get the entire manuscript up to the picture rail before he handed it in. (Adams, 2002) Hmm, isn't redrafting a literary cousin of refactoring? In many ways, I think it is - so why not apply this technique to refactoring? And we've made it so. We tied a piece of string horizontally across the wall - that's our 'picture rail'. Every week we reprint the classes we have been working on, and replace the old printouts. Then we move them up towards the string, in accordance with how happy we are with the view. Obviously, this doesn't replace all the other tools we have for evaluating code quality - e.g. the aforementioned metrics, unit tests, manual QA, and so on. It does, however, make for a brilliant way of tracking our subjective satisfaction with the class. Software quality tools can never completely replace the gut instinct of a developer - you might have massive test coverage, but that won't help with subjective measures such as code smells . With Wodehouse-style refactoring, we can now easily keep track of which code we are happy with, and which code we remain deeply suspicious of. As an added benefit, all those pages nicely cover up the hideous wall colour. Bonus!","tags":"development","title":"The P.G. Wodehouse Method Of Refactoring"},{"url":"Arthur-C-Clarke-Indistinguishable-From-Magic.html","text":"Sad news - the legendary Arthur C. Clarke has died . He'll be greatly missed; Clarke novels occupy a full shelf of my floor-to-ceiling bookcase, and Rendezvous With Rama stands proud as the finest sci-fi it has ever been my pleasure to read. Aside from his very visible mastery of sci-fi, however, there is much to remember Clarke for. He is responsible for popularising the concept of geostationary orbit , which is very important for practical global telecommunications. When you watch the Olympics on TV this summer, you can thank Clarke for the fact that you haven't had to go to China to see it. Perhaps best of all, though, is his now-infamous Third Law : Any sufficiently advanced technology is indistinguishable from magic. Clarke's Third Law has been quoted, referenced, and paraphrased copiously since he coined it, and it is almost axiomatic for many technologists. As a software engineer, I get a wry enjoyment from Gehm's Corollary, i.e. \"any technology distinguishable from magic is insufficiently advanced\" - a sobering thought when confidently hacking away on the next big thing! If your users don't think your software is magic, then you have room for improvement. I believe Clarke would have approved of that sentiment. R.I.P.","tags":"commentary","title":"Arthur C. Clarke, 16/12/1917 - 18/03/2008: Indistinguishable From Magic"},{"url":"c-30-parallel-linq-and-betfair-api.html","text":"My pal Jan has a habit of waxing lyrical about the wonders of Parallel LINQ (PLINQ) as soon as you make the mistake of mentioning multithreading within earshot. I've been playing around with .Net 3.5 recently, and I write a lot of async code day-to-day when struggling to keep desktop webservice clients responsive when making lots of webservice calls, so I thought it high time I took a closer look. The Problem A key goal for the kind of async work I do is to batch multiple calls up, so that I get all the responses at once. This is important for keeping the rest of the code clean. To illustrate, imagine you are writing an application against the Betfair API , and you have a screen that displays a market, your current profit and loss on that market, and your unmatched bets on that market. To populate this screen will require four API calls - getMarket(), getMarketPrices(), getMarketProfitAndLoss(), and getCurrentBets(). Now, the worst (though easiest) thing to do is make the four calls sequentially on the UI thread. The problem with this is it's slow, and the UI freezes during the process (since you're blocking on the UI thread), which is a lousy user experience. A slightly better approach is to spin off a thread, and make the four calls there, raising an event on completion. This gets all the work off the UI thread and therefore keeps the application responsive, but it's still slow as the calls are still sequential. To speed it up, you can create a thread per call (so four threads in this case). There's a whole lot of complexity around working out the optimum number of threads to use (depending on how many processors you have, how many simultaneous connections you are allowed to open, etc) but that's a bit beyond the scope of this post, so for now we'll go with the one-thread-per-task approach and assume it's optimal. So, each thread makes one webservice call, and raises an event to signify that it's finished. Simple, right? Unfortunately, this can lead to some real headaches in collating the data. Imagine a user has hundreds of bets on the market, and therefore the getCurrentBets() call takes a bit longer to execute than the other three. The user clicks on a market, and the threads responsible for getting market data and P&L raise their events quickly, so you display the screen with the data you have and plan to display the bets as and when they arrive. Before the bets are received, however, the user clicks on another market. Again, the market data and P&L come back quickly and you display them. Then, finally, the original getCurrentBets() call completes. But wait! You've moved onto another market now, so you don't care about those bets any more! So you have to write some code to make sure that each piece of data received is still relevant. This can become very onerous very quickly, as you struggle to determine your UI state and work out what data you want and what should be discarded. Now imagine that your application has timers firing all over the place to update prices and P&L on the market every second or two, so you have events being raised all the time. I've worked with code that ventured down this path, and believe me, you don't want to go there. The Solution The best approach is to batch these calls up, so that each happens on a separate thread, but only one event is raised - when all of the data has been received. That way, you can be sure that when you handle the event, all the data is consistent. Since this is one of the things that PLINQ does for you, it seems like a good candidate for kicking the tyres, so to speak. First, though, I'll do a quick run through of how to do this without PLINQ, for comparison's sake. The task will be to display a list of all the Premiership matches available on Betfair at the time the code runs. Take Out The Old Betfair list Premiership matches grouped by fixture date, under the Barclays Premiership node in the event tree. It looks something like this: Soccer English Soccer Barclays Premiership Fixtures 23 February Fulham v West Ham Liverpool v Middlesbrough ... Fixtures 24 February Blackburn v Bolton Reading v Aston Villa Fixtures 25 February Man City v Everton The Barclays Premiership event node has an ID that doesn't change (2022802), so I can jump straight to that node and save myself the bother of having to navigate the Soccer and English Soccer parent nodes. I'll assume you already know how to create Service References for Betfair's global WSDL, and skip straight on to creating some useful helper methods. I need to be able to call getEvents(), obviously: private GetEventsResp GetEvents ( int parentEventID ) { return m_global . getEvents ( MakeEventRequest ( parentEventID )). Result ; } private getEventsIn MakeEventRequest ( int parentEventID ) { return new getEventsIn ( new GetEventsReq () { header = new APIRequestHeader () { sessionToken = m_sessionToken }, eventParentId = parentEventID }); } If you're not used to C# 3.0, this is taking advantage of type initialisation to create nested objects without having to create a bunch of extra local variables. You can write the exact same method without type initialisation like this: private getEventsIn MakeEventRequest ( int parentEventID ) { APIRequestHeader header = new APIRequestHeader (); header . sessionToken = m_sessionToken ; GetEventsReq req = new GetEventsReq (); req . header = header ; req . eventParentId = parentEventID ; return new getEventsIn ( req ); } The first thing I need to do is get a list of fixture nodes. I can do this by asking for child events of the Premiership node, and filtering for the events that start with the word 'Fixture'. This can be achieved with a simple regex and a bit of normal LINQ: private List < BFEvent > GetPremiershipFixtureEvents () { return GetEvents ( PREMIERSHIP ). eventItems . Where ( ( ev , idx ) => Regex . IsMatch ( ev . eventName , \"&#94;Fixtures.*\" ) ). ToList (); } Assume PREMIERSHIP is a const int with the value 2022802. The Where() method works as a filter - you pass it a delegate, and it executes that delegate against each member of the list and returns a new list containing only the elements for which the delegate returned true. In this case, I'm creating the delegate with a lambda expression, which returns true for elements with an event name that is matched by the regex. Now I've got the fixture events, I need to get the child events of each, which correspond to the actual matches. I want each call to be asynchronous so that they happen in parallel, rather than sequentially. I also want to wait for all calls to complete before continuing, so I use the WaitHandle.WaitAll() method: private List < BFEvent > GetMatchEvents ( List < BFEvent > fixtureDateEvents ) { List < BFEvent > matchEvents = new List < BFEvent >(); var callbacks = ( from ev in fixtureDateEvents select StartGetEvents ( ev . eventId , matchEvents ) ). ToList (); WaitHandle . WaitAll ( callbacks . ConvertAll ( ar => ar . AsyncWaitHandle ). ToArray ()); return matchEvents ; } Here, the LINQ expression and the ConvertAll() method call are doing similar things - converting all elements of a list into another type. In the case of the LINQ expression, I am effectively obtaining a list of IAsyncResult objects by calling StartGetEvents() on each event in my list and storing the return value of each call. In the case of the ConvertAll() call, I am obtaining a list of WaitHandle objects by accessing the AsyncWaitHandle property of each IAsyncResult object in the list. It is perfectly possible to replace the LINQ expression with a call to ConvertAll(), or the ConvertAll() call with another LINQ expression. Which one you use in cases like this is largely a matter of preference. The StartGetEvents() method needs to make an asynchronous webservice call and append the results to the provided list. Since multiple threads are accessing the list, the write must be protected with a lock: private IAsyncResult StartGetEvents ( int parentEventID , List < BFEvent > matchEvents ) { return m_global . BegingetEvents ( MakeEventRequest ( parentEventID ), delegate ( IAsyncResult ar ) { lock ( matchEvents ) { matchEvents . AddRange ( m_global . EndgetEvents ( ar ). Result . eventItems ); } }, m_global ); } I am using an anonymous delegate for the callback here. All it does is lock the list and add the events contained in the response. Note that in production code you might want to be a bit more diligent about locking strategies and so on - I've written the code like this for conciseness, not necessarily for production-grade correctness. Now the whole shebang can be invoked very simply: var fixtures = GetPremiershipFixtureEvents (); GetMatchEvents ( fixtures ). ForEach ( e => Console . WriteLine ( e . eventName )); Note that the calling code is very clean and simple, and doesn't care about threads or anything like that - all that async plumbing is nicely contained in the GetMatchEvents() and StartGetEvents() methods. Bring In The New So how can PLINQ help with this? Well, it lets me get rid of those GetMatchEvents() and StartGetEvents() methods, which contain all the fiddly async code and are easily the most complex methods in the code above. First, I'll create a simple task class which represents the task of getting events for a particular ID: public class GetEventsTask { private int m_parentEventID ; private string m_sessionToken ; public GetEventsTask ( string sessionToken , int parentEventID ) { m_sessionToken = sessionToken ; m_parentEventID = parentEventID ; } public List < BFEvent > GetEvents () { BFGlobalService svc = new BFGlobalServiceClient (); APIRequestHeader header = new APIRequestHeader () { sessionToken = m_sessionToken }; return new List < BFEvent >( svc . getEvents ( new getEventsIn ( new GetEventsReq () { eventParentId = m_parentEventID , header = header })). Result . eventItems ); } } Once I've instantiated an instance of this class, a call to GetEvents() will get me all the child events for the specified parent node. To use PLINQ, all I have to do is create an array of these task objects - one per fixture date - and use the AsParallel() extension method to specify that I want the task processing done in parallel: GetEventsTask [] tasks = ( from ev in fixtureDateEvents select new GetEventsTask ( m_sessionToken , ev . eventId ) ). ToArray (); var taskResults = ( from t in tasks . AsParallel () select t . GetEvents () ). ToList (); Neat, eh? Note that PLINQ will also take care of deciding the optimal number of threads, neatly sidestepping the work I alluded to earlier. One wrinkle is that my PLINQ statement results in a list of lists, so I need to flatten it out before returning. List matchEvents = new List (); taskResults . ForEach ( results => matchEvents . AddRange ( results )); Obviously this is only scratching the surface, not only of PLINQ but of LINQ itself. Much more powerful expressions can be created with a little tweaking of the objects generated from the Betfair WSDL - but that's a topic for another article.","tags":"coding","title":"C# 3.0, Parallel LINQ, And The Betfair API - An Introduction"},{"url":"code-can-be-beautiful.html","text":"In his review of Code Is Beautiful , Jeff Atwood decides that no, actually it isn't. He's fairly adamant about it too: Ideas are beautiful. Algorithms are beautiful. Well executed ideas and algorithms are even more beautiful. But the code itself is not beautiful. The beauty of code lies in the architecture, the ideas, the grander algorithms and strategies that code represents. I just can't agree with this. It's effectively saying that a representation cannot be beautiful; only the underlying thing that's being represented can be beautiful. Worse, this argument is extended to literature and art as well, and quotes a reader review from Amazon that quotes a little Russian poetry and rhetorically asks whether any non-Russian-speaking reader can see beauty in it. This drives me nuts, it really does. Of course the representation can be beautiful, and it can also be ugly. And the beauty of the representation can have an amplifying effect on the subject of the representation. Form and content are related. A non-Russian-speaker may not appreciate Russian poetry, but that doesn't mean that form itself has no value - it means that, in this case at least, the value of form is dependent on the content. If you don't understand the content, you don't appreciate the form. This isn't an absolute, though. In literature, there are many techniques for adding value to form. Alliteration, assonance, metre, and many more techniques are all structural techniques for beautifying form. I'd argue that pretty much anyone can appreciate the compact and succinct beauty of the phrase veni, vidi, vici without understanding what it means (\"I came, I saw, I conquered\"). There are countless other examples. You don't need to understand Italian to enjoy opera, for instance. In fact, I've even heard it argued that understanding the content of an opera can diminish the experience, since the actual meaning is often fairly bland and distracts from the simple appreciation of the complex sounds and interplay of the language in the hands (or lungs) of a world-class performer. So what's the equivalent in software? I think expressiveness and elegance are key. In particular, code that is able to express ideas without adding a lot of noise. I'm very partial to Haskell for this sort of thing - for instance the canonical quicksort implementation is wonderfully precise: quicksort [] = [] quicksort ( x : xs ) = quicksort less ++ [ x ] ++ quicksort greater where less = [ y | y <- xs , y < x ] greater = [ y | y <- xs , y >= x ] If you know the quicksort algorithm, then the 2nd line of code there is about as precise an expression of the underlying concept as you could hope for. If you write the same algorithm in C or Visual Basic, I believe that you can objectively distinguish the 'beauty' of these representations of the same underlying concept. This is only possible if the representations do indeed have the quality of beauty. Another, perhaps even better, example is the naive-recursive Fibonacci generator in the same language, which is remarkably close to the mathematical definition: (from literateprograms.org ) fib n | n == 0 = 0 | n == 1 = 1 | n > 1 = fib ( n - 1 ) + fib ( n - 2 ) Note I haven't read the actual book under review here, and I have no reason to doubt the assertions that the book doesn't deliver. I do, however, take umbrage at the statement that code (or language) cannot be beautiful.","tags":"commentary","title":"Code CAN Be Beautiful"},{"url":"extending-technical-debt-metaphor.html","text":"A few months ago, the inestimable Steve McConnell (he of Code Complete fame) wrote about technical debt . McConnell looks to extend the metaphor beyond the simple idea of 'code that is going to be a liability in the future', identifying two main types of technical debt (deliberate and accidental), and identifying further correlations between the worlds of financial debt and technical debt. For instance, based on the technical debt already accumulated, one team may have a worse 'credit rating' than another: Different teams will have different technical debt credit ratings. The credit rating reflects a team's ability to pay off technical debt after it has been incurred. (McConnell, 2007) There is a lot of insight in McConnell's article, and I recommend you nip over and read it right now if you haven't already. Technical debt is indeed a useful and rich analogy for communicating a particular class of technical problem to non-technical users. I wonder, however, if McConnell hasn't extended the metaphor in slightly the wrong direction. When considering technical debt, I like to think of the product managers as the debtors, and the development team as the creditors. The actual underlying concept remains the same, it's just a shift in responsibilities. Why? As a developer, I don't always get to make the decisions about whether something should be done in a quick 'n' dirty hack, or a properly-architected solution. Of course, I'm likely to recommend the latter where I can, but it's a fact of life that I will often be overruled, and rightly so. There are occasions when incurring technical debt is the right thing to do. McConnell lists a few examples, e.g: Time to Market . When time to market is critical, incurring an extra $1 in development might equate to a loss of $10 in revenue. Even if the development cost for the same work rises to $5 later, incurring the $1 debt now is a good business decision. (McConnell, 2007) This is a key issue. Software development considerations are not the be-all and end-all, no matter how much I (or any other developer) would like them to be. It's the product teams that make these business decisions, however, and therefore it should be the product teams that incur the debt . As developers, we are the ones who give the product guys what they want, and we take on the risk of that debt not being repaid , and that's why we are the creditors. So what does this mean? It means that, when considering whether to create some additional technical debt, it's the product team that should have a credit rating. Have they been making quick-win decisions excessively over the last six months? Well then, maybe they're at their credit limit , and cannot incur any more debt until they have used some of their budget on a project that reduces debt. How about if a product manager hasn't incurred any debt recently, but made a load of bandito decisions on a major project a year ago, and now the codebase is starting to feel the impact? Charge them interest on the debt, so that now it will cost more of their budget to pay off their debt. This is entirely fair, since with a longstanding debt it is often the case that more code has been built on top of it in the interim, which may have been written well but is inherently unstable due to the shaky foundations. Paying off the debt in full will involve refactoring this new code, too. Of course, you need a fairly enlightened product team if this metaphor is to be accepted, not to mention significant buy-in from senior management if you are seriously at risk of jeopardising the product roadmap by sticking to your guns. However, since the technical debt metaphor is something of a meme at the moment, why not suggest it? If the technical debt metaphor really does improve understanding on the part of non-technical stakeholders, maybe it isn't a hopeless daydream that they'll also accept the logical extensions of the idea.","tags":"development","title":"Extending the Technical Debt Metaphor"},{"url":"reporting-on-ncover-exclusions.html","text":"On a recent project, my team was set the task of achieving 100% unit test pass-rate and code coverage. If you've ever been in this position, you'll know it's a double-edged sword - whilst it's great when the Powers That Be embrace quality instead of fixating, limpet-like, on the next deadline, it can be a nightmare when that percentage figure on the weekly summary becomes the new focus for managerial concentration, especially given how difficult it can be to hit 100%. The problem is that achieving the magical 100% is, in many cases, neither practical nor particularly useful. It can even be a problem, if the warm fuzzy feeling you get when you see \"Coverage: 100%\" leads to complacency. Even with 100% coverage and pass-rate, you don't necessarily have quality software . Our high-level project architecture involved a .Net client talking to a suite of web services written in Java. The .Net client, as an application with a GUI and a web service proxy, contained a great deal of generated code and was my main concern when the targets were set. Now, it's my belief that in most cases there's no benefit to writing tests for generated code (unless you also wrote the generator). Unless you have a very, very good reason not to, you should trust that the tools are doing their job and generating sane code. That's what they're there for. If the tools are flaky, you probably shouldn't use them at all - though I suppose that if you sometimes fell foul of a particular bug you could write a test to detect it 1 . The cause of my concern was that the UI and web reference code accounted for about 30-35% of the SLOC in the application, and so any coverage report that covered the whole app would be way short of the targets we were set. There are a number of ways to deal with this: Bite the bullet and write tests for everything . That includes InitializeComponent(), drag 'n' drop handlers, and the sync and async versions of every web service stub. Best of luck, and see you in 2017 2 . Explain patiently that some code does not need testing (or at least, is on the wrong side of the productivity bell curve and subject to massively diminishing returns in terms of effort/value). Of course, then you'll be asked to prove that you're not pulling a fast one and that the delta of your target and actual coverage percentage can be accounted for entirely by generated code. This will be tricky if you count SLOC for the generated code and use decision points for your test coverage, and maintaining this is another administrative task that you probably don't want to do. Separate your code such that some assemblies contain only generated code, and the rest contain only business logic. Then exclude the former from your test suite so they don't show on the coverage report. This is probably achievable, though it can lead to some fairly hideous contortions to maintain the boundary, and can even result in sensible design decisions being discarded in favour of wacky ones that have no redeeming feature other than supporting your arbitrary separation rules. Swear indiscriminately and refuse. Then clear your desk, probably. None of those appealed, so we set out to find another approach. What we wanted was a more flexible variant of option 3, where we could exclude methods or classes without having to exclude the whole assembly. If we could exclude code at a fairly granular level, then it became both more realistic and useful to aim for 100% coverage of our actual business code. It turns out that code exclusion isn't so tough - NCover will ignore methods and classes tagged with an attribute named CoverageExclude in the global namespace 3 . This still requires a little discipline - for example making sure that if Joe marks a class as excluded, Jim doesn't add some business logic to that class a week later without removing the attribute - but nothing that can't easily be dealt with in regular code reviews. The Powers That Be are wily, alas, and when we pitched the idea to them they approved in principle but were wary of allowing bits of code to be arbitrarily dropped off the coverage reports. If a class was excluded, who excluded it and why? This seemed reasonable for accountability - the information would be available in the source check-in notes, but that's a bit fiddly since you don't know when the attribute was added; our source control system doesn't have anything analogous to subversion's 'blame' so you have to go rummaging through a potentially very long version history. A better solution would be to find a way to add the information directly to the coverage report, so that it's right there for all to see. So, how? The first step was to get the appropriate metadata into the code. The reference implementation for the CoverageExclude attribute is as follows: public class CoverageExcludeAttribute : Attribute { } We wanted to capture additional information when the attribute was used, however, so we added a couple of read-only properties and did away with the default constructor. public class CoverageExcludeAttribute : Attribute { private string m_author ; private string m_reason ; public CoverageExcludeAttribute ( string reason , string author ) { this . m_reason = reason ; this . m_author = author ; } public string Author { get { return this . m_author ; } } public string Reason { get { return this . m_reason ; } } } Now, when anyone uses the attribute, the compiler forces them to add some additional data. [CoverageExclude(\"John Q Dev\", \"No testable code here, buster\")] public void MethodToBeExcluded ( int x , int y ) { // ... } NCover can be told to pay attention to this attribute with the excludeAttributes parameter, as explained here . With the easy bit out of the way, the next task was to report on these exclusions. Our build system, after running the test suite, used NCoverExplorer to generate a summary report. You can tell NCoverExplorer to list exclusions in reports, so we figured that would be a good place to start. The appropriate NAnt incantation is: <ncoverexplorer failonerror= \"false\" program= \"C:\\NCoverExplorer\\NCoverExplorer.Console.exe\" projectName= \"Atmosphere Processor::LV426\" reportType= \"4\" xmlReportName= \"Report.xml\" mergeFileName= \"CoverageMerge.xml\" showExcluded= \"True\" satisfactoryCoverage= \"80\" > <fileset> <include name= \"Coverage.xml\" /> </fileset> <exclusions> <exclusion type= \"Assembly\" pattern= \"*Tests\" /> <exclusion type= \"Assembly\" pattern= \"*Fixtures*\" /> </exclusions> </ncoverexplorer> Note the reportType and showExcluded attributes, which specify the summary report we want, with details of excluded code appended to the report. Note also the exclusion nodes, which specify that we want our test assemblies excluded from coverage metrics. The report will include a table like this: Our goal was to somehow get our custom properties (Author and Reason) into this report. To do so, firstly we needed to modify the above table with two extra columns to hold this custom data. NCoverExplorer ships with stylesheet called CoverageReport.xsl; the table modification was achieved by tweaking the 'exclusions summary' section as follows: <!-- Exclusions Summary --> <xsl:template name= \"exclusionsSummary\" > <tr> <td colspan= \"5\" > &#160; </td> </tr> <tr> <td class= \"exclusionTable mainTableHeaderLeft\" colspan= \"1\" > Excluded From Coverage Results </td> <td class= \"exclusionTable mainTableGraphHeader\" colspan= \"1\" > All Code Within </td> <td class= \"exclusionTable mainTableGraphHeader\" colspan= \"2\" > Reason For Exclusion </td> <td class= \"exclusionTable mainTableGraphHeader\" colspan= \"1\" > Developer </td> </tr> <xsl:for-each select= \"./exclusions/exclusion\" > <tr> <td class= \"mainTableCellBottom exclusionTableCellItem\" colspan= \"1\" > <xsl:value-of select= \"@name\" /></td> <td class= \"mainTableCellBottom mainTableCellGraph\" colspan= \"1\" > <xsl:value-of select= \"@category\" /></td> <td class= \"mainTableCellBottom mainTableCellGraph\" colspan= \"2\" > <xsl:value-of select= \"@reason\" /></td> <td class= \"mainTableCellBottom mainTableCellGraph\" colspan= \"1\" > <xsl:value-of select= \"@author\" /></td> </tr> </xsl:for-each> </xsl:template> The next step was to actually inject our custom data into the report. This was a two-stage process: Use reflection to iterate through the application assemblies, looking for anything tagged with our attribute Open the report data file generated by NCoverExplorer and shoehorn our new data into it. We created a simple little post-processor application to perform this work. To complete stage 1, we needed to iterate through a directory of assemblies, loading each one in turn. In each assembly, we iterated through the types contained therein, and looked for our custom attribute on each one. Then, we iterated through the methods on each type, and looked for the custom attribute there too. This is actually very simple - the code skeleton looks like this: foreach ( FileInfo assemblyFile in assemblies ) { try { // Attempt to load the file as an assembly, and grab // all the types defined therein Assembly assembly = Assembly . LoadFrom ( assemblyFile . FullName ); Type [] types = assembly . GetTypes (); // Spin through the types, looking for classes and // methods tagged with CoverageExclude foreach ( Type type in types ) { object [] classAttributes = type . GetCustomAttributes ( typeof ( CoverageExcludeAttribute ), false ); foreach ( CoverageExcludeAttribute classAttribute in classAttributes ) { // ... } MethodInfo [] methods = type . GetMethods ( BindingFlags . Public | BindingFlags . NonPublic | BindingFlags . Instance | BindingFlags . Static ); foreach ( MethodInfo method in methods ) { object [] methodAttributes = method . GetCustomAttributes ( typeof ( CoverageExcludeAttribute ), false ); foreach ( CoverageExcludeAttribute methodAttribute in methodAttributes ) { // ... } } } } catch ( Exception ex ) { // Probably not a .Net assembly, do some appropriate // complaining to the user } } In the loops, we cached the fully-qualified names of the types and methods tagged with the attribute. Stage 2 was implemented by tweaking the XML data file NCoverExplorer generates for the report. This is straightforward too - suck the report into an XmlDocument, grab the exclusion nodes, and add a couple of attributes to each one. All the types and methods were already listed since we'd set the excludeAttributes parameter in the NAnt configuration (see above). Therefore, all we needed to do was match up the FQNs we cached in stage 1 with the nodes already in the report: XmlDocument doc = new XmlDocument (); doc . Load ( coverageFile . FullName ); // Go through all the exclusion nodes and try to match // them up against the exclusion data we have sucked // out of the assemblies foreach ( XmlNode node in doc . SelectNodes ( \"//exclusion\" )) { switch ( node . Attributes [ \"category\" ]. Value ) { case \"Class\" : // Find and remove the first exclusion reason for // this class FindExclusionAndModifyNode ( exclusions . ClassExclusions , node ); break ; case \"Method\" : // Find and remove the first exclusion reason for // this method FindExclusionAndModifyNode ( exclusions . MethodExclusions , node ); break ; default : // Exclusion at either assembly or namespace level break ; } } The implementation of FindExclusionAndModifyNode simply loops through the cached FQNs to see if we have data that corresponds to the current node, and if so it creates two new attributes - one containing the name of the developer that added the CoverageExcludeAttribute to the code, and another containing their justification for doing so. Then the modified XmlDocument is written out to disk, overriding the original. The end result is a report that looks something like this , with all the excluded code neatly documented to keep suspicious managers happy. Since the post-processor was written as a simple command-line application, we could create a custom NAnt task for it and integrate the whole process seamlessly with our continuous integration setup. I've seen it happen a few times before, for example an XML generator (which shall remain nameless) that occasionally 'forgot' our custom namespace and used a default, which caused parsers of that XML to scream in agony. It's rare though, unless you regularly dig up tools from CodeProject and use them in your production code, in which case you deserve everything you get ;-) ↩ Written in 2008. So if you're reading this on December 31st 2016, adjust accordingly and don't come crying to me. ↩ Yes, I know that NCover 2.x has built-in regex-based exclusions that do all this, but a) not everyone has an NCover 2.x pro licence, and b) we weren't using NCover 2.x as it hadn't been released at the time. ↩","tags":"tools","title":"Reporting on NCover Exclusions"},{"url":"descent-into-incompetence.html","text":"I am fairly heavily involved with recruitment where I work, being the author of the technical test and phone screen questions we use for evaluating candidates, and conducting face-to-face interviews with many of the hopefuls that get over these early hurdles. Naturally, in order to gain these responsibilities I have gone through a number of required HR ass-covering exercises in which it was drilled into me that I am legally forbidden from asking questions about sexuality, marital status, family-planning, and anything else which might lead me into rejecting a candidate on grounds our beloved government considers discriminatory. Never mind that I have never shown the least inclination to discriminate against someone because they might want to possibly think about maybe taking some [mp]aternity leave in the next 30 years, or (gasp) prefer the company of their own gender, or whatever; I have to go through all this training so that the company can throw me to the wolves if a candidate claims to have been discriminated against. \"Not our fault, guvnor; we explained the rules\". Still, fair enough I suppose; we live in litigious times, and not being a bigot I have no particular fears of transgressing. But what if the rules are changed? And what if they're changed in horribly unexpected ways? A recent article on the BBC News site contained, quite without fanfare, some shocking intelligence. Previously standard questions about age, length of experience and religious views are now illegal, Which? points out. Wait, what? Length of experience is now a forbidden topic? So if I'm recruiting a senior developer or team lead, I now have to waste valuable time interviewing fresh-out-of-college tyros who haven't written a single line of commercial code or spent a single day working in a professional team? I can kind of see what is trying to be achieved here, but it is an unavoidable fact that experience is a vital attribute for many senior roles, and needs to be taken into consideration when trying to fill those roles. It's not just me either - a quick trawl through the endless agency emails I seem to get every day (despite telling them I'm not on the market) reveals that most tech jobs are still specifying n years of experience; this seems somewhat pointless now that candidates can't be asked about it. I wonder if they know? Even more interesting is the fact that many contract positions are still paid at 'rates negotiable on experience'. Hah, how does that work when experience is a forbidden subject? If I were graduating from university this year I'd be whoring myself around the City applying for £500-per-day contracting gigs and suing any bank that dared ask me to justify my rate. Rob Grant's novel Incompetence just became slightly less hysterical. Article 13199 of the Pan-European Constitution: \"No person shall be prejudiced from employment in any capacity, at any level, by reason of age, race, creed or incompitence ( sic ).","tags":"commentary","title":"Descent Into Incompetence"},{"url":"freedom-zero-all-or-nothing-fallacy.html","text":"Jeff Atwood has an article up today bemoaning the fact that seemingly nobody \"gives a crap about freedom zero\". Well, my initial reaction was that surely nobody could care about something with such a thoroughly ridiculous name. Freedom Zero? Really? I know this is the FSF 's first freedom, and programmers count from 0 don'tcha know, but it's still rubbish. But the real reason is that it simply isn't important enough to override everything else. Certainly, for some things you want the freedom and reliability of open source. I write my essays in OpenOffice, I use Vim as my text editor for all programming languages other than C#, and I write maths papers with LaTeX. I want my personal output to remain usable and not at the whim of some company somewhere, I agree with all that. But do I need my MP3 player to be open? No. My videogame console? No. My phone? No. My movie editor? No (though only because I always archive the source material). The irony is that people do indeed care about freedom - the freedom to choose , and the sad fact is that there is a certain type of zealot who only espouses freedom as long as it's their type of freedom . And that isn't freedom at all. Now, as it happens, Linux is my operating system of choice. I don't own any Apple computers, though I do have a first-gen iPod Mini, which is distinctly showing its age. I use Vista for .Net development, but I don't think anyone could reasonably call me an Apple zealot or an anti-freedom capitalist whatever. But you won't catch me criticising Apple for their closed platform. If it results in a decent product, I'm all for it. I used to have a G4 iBook and liked it a lot. When I'm in the market for an ultraportable later this year, I will give due consideration to the Mac Air. A Mac is a product - calling the hardware nothing more than a dongle is a ridiculous argument. You can run Linux on Mac hardware, and OSX on non-Mac hardware (suboptimally, granted). Would you call a Ferrari Enzo a dongle because you need one in order to run the Enzo engine management software? And it should be said that Apple isn't quite as closed as some people suggest - I can still install Firefox, Thunderbird, and other open source tools if I want to tackle the hostile internet with a trusted armoury. And OSX comes with things like Apache and SSH installed out of the box. So do I give a crap about freedom zero? Only in as far as it suits my needs. If a piece of closed software does a better job, and the risk of losing data forever is within my tolerances , then sure I'll use it and I won't let ideology get in my way. On the flip side, I care about interoperability, and I contribute or donate to a few open source projects, and will strongly oppose anything - legal or technological - that attempts to muscle open source out of existence. An open source tool deserves the right to compete. I use Amarok not because it's open, but because I like it more than iTunes . Conversely, I use Visual Studio not because it's proprietary, but because I prefer it to SharpDevelop . Use the best tool for the job.","tags":"commentary","title":"Freedom Zero: The All-Or-Nothing Fallacy"},{"url":"technical-book-club-object-oriented.html","text":"So, as previously mentioned , we'll start with the basics. This material is probably very familiar to most coders with even a small amount of experience, but it never hurts to refresh the fundamentals. You may even find that there's some material that seems so obvious you don't even actively think about it any more - which is good if it has become habit, but may be bad if you've grown complacent in certain areas. The overarching theme of the first chapter is complexity . Complexity is the enemy of the software developer, and it is vital to understand this fact. If you don't identify complexity as the enemy, you will find it harder to remain vigilant against it. Anyone who has worked as a developer for more than a year or two will almost certainly have exposure to the problems caused by complexity, but quite often complexity itself will not be pinpointed as the root cause. It may be a case of not seeing the wood for the trees - when trying to understand a system it's very easy to get bogged down wondering why some particular section of code does things a certain way, and not see the problems with the big picture. Of course, since complexity obscures the big picture, this is common and self-perpetuating. Object-oriented design, then, is a tool for managing complexity. Of course, it is many other things too, but this is one of the fundamentals. In particular, OOD is a natural way to represent hierarchies , and hierarchies are the primary tool Booch presents for making complexity manageable. A number of examples from nature are provided; for example, you can view a plant simply as a plant, or as a collection of structures (leaves, stem, roots). Importantly, the overall hierarchical view of a plant can be broken down into many interacting sub-hierarchies, each of which may be considered in terms of its own structure and its interactions . This is an example of decomposition . If you want to study roots in detail, you can study the branch roots, the root apex, and the root cap - and break that down further if you like to consider roots as a collection of cells. To study roots in this sort of detail, however, you do not have to go to the same lengths with leaves and stems - it is enough to understand the interactions between the higher- level components. Complexity, therefore, is more manageable if it is divided into interacting components, each of which can be further divided into interacting subcomponents. At different levels of abstraction there are clear boundaries - it shouldn't be necessary to understand the epidermis of a leaf to examine a root, and likewise the study of a leaf should not need to consider the role of the root apex. This is known as separation of concerns , and allows you to ignore the parts of the system that you are not interested in at the time. In software, these principles are captured by OOD. Broadly, hierarchies can be modelled with inheritance, components can be modelled with modules, intercomponent interactions can be modelled with interfaces and method calls, and intracomponent interactions can be modelled with aggregation. These interactions are key, as they form part of the 'value' of a system - in layman's terms, the whole is greater than the sum of its parts. Inheritance and aggregation are, respectively, 'is-a' and 'part-of' relationships. In both cases, these represent separate but overlapping hierarchies. Booch's example is that of an aeroplane. An aeroplane can be thought of as an aggregation of systems - propulsion, flight control, etc. Each of those can potentially be modelled as specilaised types too - for instance a jet engine is a particular type of propulsion, and a turbofan engine is a particular type of jet engine. In OO terms, the 'is-a' relationships are expressed as class structures utilising inheritance, and 'part-of' relationships are expressed as object structures utilising aggregation. Relative primitives are an interesting concept, and refer as much as anything to the benefit of having a sensible and appropriate vocabulary at each level of abstraction. With plants, for instance, if you are working at the cellular level your primitives are nuclei, chloroplasts etc. If, however, you are at the top level your primitives should be leaves and stems - something is wrong if you are concerning yourself with the nucleus of a cell at this level of abstraction. Relative primitives are a natural consequence of hierarchies and decomposition, if your hierarchies are sane. Even with all these tools for managing complexity at our disposal, it is still extraordinarily hard - if not nigh-on impossible - to construct a large complex system in one fell swoop. Booch identifies that a key characteristic of successful complex systems is that they evolve from simpler systems, whilst always being useful during that evolution - they have stable intermediate forms . Contemporary development processes, such as agile development, explicitly recognise this with the mantra of 'deliver early, deliver often'. The idea is that functionality should be delivered iteratively, with each iteration being functional and testable. This is in contrast to more traditional waterfall methodologies, which are notoriously associated with failed projects, often due to attempting to design a large complex system up-front and develop it all at once. In summary, then, Booch argues that the characteristics of a manageable complex system are as follows: Hierarchic Uses relative primitives Has robust separation of concerns Has stable intermediate forms It is important to note in passing that, since OOA&D is an unashamed champion of object-orientation as a mechanism for managing complexity, OO is by no means the only approach - functional and procedural languages have their own techniques, and in some cases may be more suitable, depending on the problem to be solved. Even so, many of these principles are common and are useful things to bear in mind when designing a system. Next week we look at chapter 2, which covers the object model in greater detail.","tags":"bookclub","title":"Technical Book Club: Object Oriented Analysis & Design, Chapter 1"},{"url":"fab-fib.html","text":"Scott Hanselman continues his Weekly Source Code series with a look at algorithms for generating the Fibonacci sequence in a variety of different languages . He misses my favourite implementation, though fortunately a wise commenter has already sought to correct such an egregious error. As is so often the case, it is Haskell that provides the most elegant yet mind-bending alternative: fibonacci n = fib !! n where fib = 0 : 1 : zipWith ( + ) fib ( tail fib ) This little beauty appends a list with its own tail while it's still being generated and lazily sums the elements. On top of that, it's fast , since unlike most naive recursive Fibonacci generators it doesn't waste time recalculating previous values. In fact, it runs in linear time, that is O(n) , and on a fairly modest 2GHz Athlon XP will calculate the 50,000th Fibonacci number in around 600 milliseconds. By contrast, the naive recursive implementation in C# (see below, adapted and corrected from the code Scott posted) takes 28 seconds to calculate the 45th number, on a much more powerful Core Duo machine. $ time ./Fibs.exe 1134903170 Execution time: 00 :00:28.2930000 real 0m28.382s user 0m0.000s sys 0m0.031s As implemented, you can't go much higher than this since the 47th number in the sequence (2971215073) is too big to store in a 32-bit signed int. Asking for the 50,000th number results in an immediate stack overflow, which is the runtime's way of saying \"don't be ridiculous, mate\". Of course, the C# version could be made many times faster and more efficient by implementing it iteratively (i.e. with a for loop), but this is less natural since the Fibonacci sequence is a recurrence relation and therefore best expressed recursively. The beauty of the Haskell version is that it combines expressiveness with performance, always a happy combination. using System ; namespace Fibs { class Program { static void Main ( string [] args ) { DateTime start = DateTime . Now ; Console . WriteLine ( Fibonacci ( 45 )); Console . WriteLine ( \"Execution time: {0}\" , DateTime . Now . Subtract ( start )); } static int Fibonacci ( int n ) { if ( n == 0 || n == 1 ) return n ; return Fibonacci ( n - 1 ) + Fibonacci ( n - 2 ); } } }","tags":"commentary","title":"Fab Fib"},{"url":"turbocharging-net-webservice-clients.html","text":"Since the first version of .Net and its associated toolset, Microsoft have sought to make it easy to write SOAP services and SOAP clients. And, generally, they have succeeded quite well. Whilst the open-source world has tended to prefer the simpler REST approach, MS (and Sun, and Apache) have done an admirable job of taking the large, complex SOAP protocol and making it reasonably straightforward to work with most of the time. One of the areas in which things get somewhat less straightforward is high performance. Granted, most web services don't have particularly eye-popping requirements in terms of hits or transactions, but occasionally you find an exception. Betfair , for instance, have an API that has peak rates in excess of 13,000 requests per second , many of which hit the database, with individual users making tens or even hundreds of requests per second. Betfair's data changes at a breathtaking rate and there is a perceived advantage to getting hold of up-to- the-millisecond information. To put that in context, the Digg Effect is estimated to peak at around 6K-8K hits per hour as I write in December 2007, which translates to a piffling couple of hits per second (8000 / 60 / 60 = 2.2). Even a more extreme prediction of 50K hits per hour is only around 14 hits per second, so we're talking about handling three orders of magnitude more requests than Digg generates. If you are writing a .Net client and want to get the best out of this sort of situation, you are hamstrung unless you learn a few tricks. Read on to learn five of the best. I'll be using the Betfair API as an example throughout, but the techniques apply to any high-performance web service where the usage profile involves frequent small (<1KB) SOAP requests. For those who normally turn to the back of the book for answers and don't really care about the whys and wherefores, here's the executive summary: Switch off Expect 100 Continue . This should be done in your App.config file (see below). Switch off Nagle's Algorithm . This should be done in your App.config file (see below). Use multithreading . Unfortunately this is not a simple configuration file setting, it is a fundamental part of your application design. Remove the maximum connection bottleneck . This should be done in your App.config file (see below). This is vital if your application is multithreaded. Use gzip compression . .Net 2 has this built-in if you switch it on; .Net 1.1 needs a helping hand. The XML snippet that configures these settings is shown below, and should be added to your App.config file: <system.net> <connectionManagement> <add address= \"*\" maxconnection= \"20\" /> </connectionManagement> <settings> <servicePointManager expect100Continue= \"false\" /> <servicePointManager useNagleAlgorithm= \"false\" /> </settings> </system.net> For those who share my distrust of unexplained sorcery , here's the gory details. Expect-100 the Unexpected RFC 2616 (the specification for HTTP 1.1) includes a request header and response code that together are known as 'Expect 100 Continue'. When using the Expect header, the client will send the request headers and wait for the server to respond with a response code of 100 before sending the request body, i.e. splitting the request into two parts with a whole round-trip in- between. Why would you ever want to do this? Well, imagine you are trying to upload a 101MB file to a web server with a 100MB file size limit. If you just submit the whole thing as one request, you'll sit there for ages waiting for the upload to complete, only to have it fail right at the last minute when you hit the 100MB limit. Using Expect 100 Continue, you submit just the headers initially, and wait for the server's permission to continue; in our example, this gives the server a chance to look at the Content-Length parameter and identify that your file is too big, and return an error code instead of permission to continue. This way you know the upload will fail, without needlessly transmitting a single byte of the large file. By default, the .Net Framework uses the Expect 100 Continue approach. Most SOAP requests are not, however, anywhere near 101MB in size, and a server designed to deal with thousands of requests per second is not likely to be returning anything other than 100 Continue if the Expect header is sent. Depending on latency, the round-trip penalty may be unacceptable. For example, Betfair's Australian exchange (physically located in Australia) contains all their Australian markets, so if you're in the UK and want to trade on the Australian Open tennis you're subject to a round-trip time of about 350ms. If you allow your requests to be split into two, then you have two round-trips, so your request will take 700ms plus processing time. You don't want that, so switch it off. Note that the request headers and body are still sent separately, but the latter is no longer dependent on the response to the former (so they are shown as sent together in this diagram). The setting corresponding to this in the XML snippet above is: <servicePointManager expect100Continue= \"false\" /> Nagle's Algorithm Nagle's algorithm is a low-level algorithm that tries to minimise the number of TCP packets on the network, by trying to fill a TCP packet before sending it. TCP packets have a 40-byte header, so if you try to send a single byte you incur a lot of overhead as you are sending 41 bytes to represent 1 byte of information. This 4000% overhead causes chaos on congested networks. A TCP packet size is configurable, but is often set to 1500 bytes, and so Nagle's algorithm will buffer outgoing data in an attempt to send a small number of full packets rather than a huge amount of mostly empty packets. Nagle's algorithm can be paraphrased in English like this: If I have less than a full packet's worth of data, and I have not yet received an acknowledgement from the server for the last data I sent, I will buffer new outbound data. When I get a server acknowledgement or have enough data to fill a whole packet, I will send the data across the network. Now, with our use case (frequent SOAP requests, each <1KB) a request doesn't fill a packet, so requests are subject to buffering. Furthermore, as explained above, even with Expect 100 Continue switched off the request headers are still sent separately from the request body, so the body of the first request is buffered until the request headers reach their destination and the server sends back a TCP ACK. Let us again consider a UK client communicating with Betfair's Australian API. Your application issues two requests one for current prices and one for your current market position (Req1 and Req2 in the diagram below). Together, both these requests are less than 1500 bytes. The headers for the first request are transmitted, but Nagle's algorithm buffers the request body, plus the whole second request, since they don't fill a TCP packet. Due to the round trip latency, the acknowledgment of the first request's headers is not received for 350ms, so that is how long the requests are buffered for. When the requests do get sent, they too are subject to 350ms latency around the world, so again you end up with around 700ms added to each of your Australian API calls. Without Nagle, we dispense with the buffering and send out our smaller packets immediately, saving a round-trip. One word of warning - disabling Nagle can cause problems if you are on a highly-congested network or have overworked routers and switches, since it increases the number of network packets flying around. If you don't own your network, or are deploying your web service client to a large number of users, you might want to think about this carefully as you won't be popular if your software saturates the network. The setting corresponding to this in the XML snippet above is: <servicePointManager useNagleAlgorithm= \"false\" /> Happenin' Threads This one should be pretty obvious. A single-threaded application can only do one thing at a time. A multi-threaded application can do multiple things at a time. So if, for example, you want to display some information and need to collate the results from four web service requests to build your view, calling them one after another is going to be slower than calling them all simultaneously (not to mention freezing your UI if you're writing a WinForms application and making calls on the UI thread). Explaining how to design and write multi-threaded .Net applications is way out of scope for this blog post, but any time you spend reading and learning about it either online or in books is going to be time well-spent. Go on, do it now. Also read the next section , otherwise your application will not get as much benefit as you expect. Walk and Chew Gum .Net, by default, has a bottleneck on simultaneous web service calls against the same host. This one catches loads of people out - they write clever multi-threaded applications that issue many, many requests, and never realise that beneath the application layer a lot of their requests are called in sequence rather than in parallel. Tsk, damn Microsoft for unnecessarily crippling their framework, right? Well no, actually, since this is an example of Microsoft following standards and recommendations and is to be applauded, lest they go back to their old ways of \"embrace and extend\". The recommendation in question is from RFC 2616 again, and states: A single-user client SHOULD NOT maintain more than 2 connections with any server or proxy. ( RFC 2616 , section 8.1.4) .Net uses HTTP 1.1 persistent connections by default (this is a good thing - you don't want to incur the cost of establishing a TCP connection with every request, especially if you have long round-trip times as well since it involves multiple round-trips), so MS have done the right thing and restricted processes to two simultaneous connections by default. What does this mean for your application? It means that if you want to make 20 requests, and do so on 20 threads as recommended above, under the hood .Net only makes two at a time and queues the rest up. Therefore your 20 threads are wasted, as your throughput is no better than if you'd only created two threads, and innocent web servers are protected from overzealous clients. When we know that we're talking to an insane city-flattening Godzilla-on-steroids of a server, however, the two-connection limit is unreasonable and we can ramp-up safely. The corresponding setting in the XML snippet above is: <connectionManagement> <add address= \"*\" maxconnection= \"20\" /> </connectionManagement> Sorted. You can, of course, change the number 20 to whatever is appropriate for your application. Warning: Do not, under any circumstances, get into the habit of configuring this for all your web service applications. With all 20 threads issuing one request per second, you are exceeding the 14-request-per-second Digg Effect example I used above; with this technique and enough bandwidth your application will be quite capable of taking a lot of websites down, and those that manage to weather the storm will probably blacklist your IP address and/or close your account. Only use this if you are absolutely sure the server is up to it and such aggressive behaviour is permitted by the owners . Small is Beautiful The last step is to enable compression on your responses. Depending on the nature of the service you are using, the value of this tip may vary, but it's likely to be of some benefit given the number of requests per second we are issuing. Of course, it is dependent on the web service actually supporting compression, but it's been a standard feature of just about every web server for years, so this shouldn't be a problem. Lets look at a worked example. The getMarketPrices response from the Betfair API is about 30KB of XML. If, as above, we have 20 threads issuing one request per second, and each thread is interested in prices from a different market, that's about 600KB of data per second, which will quite easily saturate a lot of home broadband connections. With gzip compression, however, each response comes down to about 5KB (XML compresses very well, generally, since it is just text with a lot of repetition and whitespace), so the 20 threads now demand a more manageable 100KB per second. Great. So how do we use it? In .Net 2.0 and above it's very easy - just set the EnableDecompression property of your web service proxy object (ignore IntelliSense, which incorrectly claims the value is true by default; it's actually false by default, as stated on MSDN . For example, to get compressed responses from Betfair's global server : BFGlobalService service = new BFGlobalService (); service . EnableDecompression = true ; If you're still using .Net 1.1, you have a bit more work to do, since support for gzip was inexplicably left out of the framework. First, you need to subclass the generated BFGlobalService proxy class, and override some key methods so you can a) include the Accept-Encoding header to tell the server that you understand gzip, and b) decompress the gzipped response before the XML deserializer sees it, otherwise it'll choke. public class BFGlobalWithGzip : BFGlobalService { /// <summary> /// Adds compression header to request header /// </summary> protected override System . Net . WebRequest GetWebRequest ( Uri uri ) { HttpWebRequest request = ( HttpWebRequest ) base . GetWebRequest ( uri ); // Turn on compression request . Headers . Add ( \"Accept-Encoding\" , \"gzip, deflate\" ); return request ; } /// <summary> /// Decompress response before the Xml serializer gets /// its hands on it /// </summary> protected override WebResponse GetWebResponse ( WebRequest request ) { return new HttpWebResponseDecompressed ( request ); } /// <summary> /// Need to override this method if performing /// asynchronous calls, otherwise de-compression /// will not be performed and will throw an error /// </summary> protected override WebResponse GetWebResponse ( WebRequest request , IAsyncResult result ) { return new HttpWebResponseDecompressed ( request , result ); } } Next, implement the HttpWebResponseDecompressed class. This subclasses .Net's WebResponse class and knows how to decompress a response if it has ContentEncoding 'gzip': public class HttpWebResponseDecompressed : WebResponse { private HttpWebResponse m_response ; private MemoryStream m_decompressedStream ; public HttpWebResponseDecompressed ( WebRequest request ) { m_response = ( HttpWebResponse ) request . GetResponse (); } public HttpWebResponseDecompressed ( WebRequest request , IAsyncResult result ) { m_response = ( HttpWebResponse ) request . EndGetResponse ( result ); } public override long ContentLength { get { if ( m_decompressedStream == null || m_decompressedStream . Length == 0 ) return m_response . ContentLength ; else return m_decompressedStream . Length ; } set { m_response . ContentLength = value ; } } public override string ContentType { get { return m_response . ContentType ; } set { m_response . ContentType = value ; } } public override Uri ResponseUri { get { return m_response . ResponseUri ; } } public override WebHeaderCollection Headers { get { return m_response . Headers ; } } public override System . IO . Stream GetResponseStream () { Stream compressedStream = null ; if ( m_response . ContentEncoding == \"gzip\" ) compressedStream = new GZipInputStream ( m_response . GetResponseStream ()); else if ( m_response . ContentEncoding == \"deflate\" ) compressedStream = new InflaterInputStream ( m_response . GetResponseStream ()); if ( compressedStream != null ) { m_decompressedStream = new MemoryStream (); int size = 4096 ; byte [] buffer = new byte [ size ]; while ( true ) { size = compressedStream . Read ( buffer , 0 , size ); if ( size > 0 ) m_decompressedStream . Write ( buffer , 0 , size ); else break ; } m_decompressedStream . Seek ( 0 , SeekOrigin . Begin ); compressedStream . Close (); return m_decompressedStream ; } else return m_response . GetResponseStream (); } } To decompress the data, we need a decompression library since .Net 1.1 doesn't provide one. In most cases, SharpZipLib will do the business: using ICSharpCode.SharpZipLib.GZip ; using ICSharpCode.SharpZipLib.Zip.Compression.Streams ; Now, when creating an instance of BFGlobalService you can use the gzip-supporting subclass and everything else happens automatically. BFGlobalService service = new BFGlobalServiceWithGzip (); Fin Jebus, that went on for a bit. Now, go forth and write high-performance clients at will - but heed the warnings about only doing this when you know the server is on-the-ball, because these tips really can cause havoc if used irresponsibly.","tags":"coding","title":"Turbocharging .Net Webservice Clients"},{"url":"legacy-code-refactoring-and-ownership.html","text":"Refactoring is good. Everyone knows that. Since Fowler popularised the concept with the seminal Refactoring it's become a staple of the industry, and has pride of place on many a bookshelf. In the many, many articles and discussions of the subject, the key goals and benefits of refactoring are generally taken to be the improvement of readability, testability, decoupling, and other similar worthy ideals. For me, however, there is another very distinct benefit, often overlooked. Fowler touches upon it, but doesn't really develop it, early on in Refactoring : I use refactoring to help me understand unfamiliar code. When I look at unfamiliar code, I have to try to understand what it does. I look at a couple of lines and say to myself, oh yes, that's what this bit of code is doing. With refactoring I don't stop at the mental note. I actually change the code to better reflect my understanding, and then I test that understanding by rerunning the code to see if it still works. (Fowler, Refactoring , 1999) By investigating a piece of code thoroughly enough to understand how it works, refactoring it to map directly on to your understanding, and reinforcing everything with good unit tests, you take ownership of the code. It's yours now. This is very important, psychologically. Almost every developer feels more at home with their own code than somebody else's. That's why you feel uncomfortable and deflated when, 20 minutes into deciphering a nasty bit of opaque gibberish, you realise it was something you yourself wrote a year earlier and subsequently forgot about. When you refactor, you rewrite code to a greater or lesser extent. Having done so, the resulting feeling of ownership (alongside increased understanding, of course) makes the code much less scary. The benefit of this is less marked in agile methodologies or TDD, of course, since in those cases quite often the code you are refactoring was written by you anyway. Working with legacy code, though, it's a big deal. In the preface to Working Effectively With Legacy Code , Feathers asks \"what do you think about when you hear the term legacy code ?\" (Feathers, 2004). He answers by stating that the standard definition is \"difficult-to- change code that we don't understand\" and adds his own preferred definition which is, in essence, \"code without tests\". My own definition of legacy code would include, in many cases, code that isn't mine . By 'mine' I don't exclusively mean code I wrote personally; I also mean code written by my team, or even code written by people who sit a couple of desks down who I can go and pester about it (which is stretching the definition a bit, admittedly). In short, legacy code for me is code that no longer has any accessible owner. Like a stray cat or dog, code without an owner goes feral. Refactoring is the process of taming feral code, but as with stray cats much of the benefit comes from re-homing. This is a vital process, even if a fairly unconscious one. When you first come face to face with some hideous 5000-line spaghetti monster of a function your heart sinks - how can anyone ever hope to understand that, let alone modify it safely? Especially if the only people that ever worked with it left the company 3 years ago? Refactoring allows you to split this code up, create classes to better represent the problem domain, improve abstraction, add tests, and all that other good stuff; at the same time, the process of doing so makes the code yours. You make the decisions about the classes to create and the abstractions to introduce. You write the tests that ferret out all the little idiosyncrasies, and uncover the unwritten assumptions. By the end of the process, the code feels like yours. And that means that the next time you have to make a change there, you benefit from the double whammy of code that is not only well-written and tested, but recognisably yours ; and that's the kind of code that you won't mind working with.","tags":"development","title":"Legacy Code, Refactoring, and Ownership"},{"url":"coding-by-convention.html","text":"I've been meaning for a while to have a play around with Ruby on Rails , on the basis that anything generating so much hype over the last year or two deserves some level of investigation, if only to see whether the hype is justified. So, I spent a couple of days working through Agile Web Development with Rails and, well, it's pretty nice. I can certainly appreciate a development environment that goes to such endearing effort to do work for you without getting in the way - a fairly tricky balancing act. I came to the book with a working knowledge of Ruby but zero practical exposure to Rails, and on top of that I'm not a web developer so could not bring much contextual experience to the table. Despite this, I worked through the book and ended up with a functional book-store application in about 15 hours. Not too shabby. So how does Rails achieve such power and productivity? The answer is largely that Rails, more so than pretty much any other development environment I've used, leverages the power of convention. That is, if you stay 'on rails' and behave the way Rails wants you to, then in return you get a great deal of functionality for free. A kind of technological \"you scratch my back, and I'll scratch yours\". If you structure your application as Rails expects, then Rails will automatically hook everything up for you. If you name your database tables as Rails wants you to, and create the primary/foreign key id columns that Rails expects, then Rails will take care of all your object-relational- mapping needs for you. Sounds like a good deal, yes? Rails doesn't expect you to jump through all these hoops yourself though. It provides a number of useful scripts that you can use to perform the common tasks you want to do, in the way that Rails wants you to do them. Probably the best example of this is when you first start a new project. You ask Rails to create an application for you, with the name you specify, then off it goes - and creates 45 files in 37 directories, without you having to lift a finger. $ rails dummy create create app/controllers ... ... create log/development.log create log/test.log $ find dummy/ -type f | wc -l 45 $ find dummy/ -type d | wc -l 37 Compare this to a newborn ASP.Net application created using the Web Site wizard in Visual Studio 2005: $ find WebSite1/ -type f | wc -l 2 $ find WebSite1/ -type d | wc -l 2 A pretty substantial difference. And if you stay within the confines of Rails' expectations when adding to the project - which is very easy to do since you are provided with more generators for creating models, controllers, and migrations (basically incremental DB deployment scripts) - then you end up with a nicely structured application in accordance with the hallowed principles of MVC design, and everything is glued together automatically. Create a new data model, and your controller is immediately able to load it from the database along with all its relational buddies in a nice aggregated object structure with just one line of code (as long as you remembered to add all the has_many and belongs_to calls, of course). Store that data object in a controller member variable, and your views can access it for display. Use one of the magical rake incantations and get a DB-backed session management system which will horizontally scale in a load-balanced environment. Run the script/console script and you are dropped into a fully interactive command- line environment similar to irb, where you can instantiate and interact with all your objects dynamically. Tail the development log and you can see all the generated SQL as it is executed, and even get indicated performance in terms of theoretical request-per-second capacity. It's all just fab. Nothing spectacularly new, of course; each individual feature has been done before, but Rails pulls them all together very nicely indeed. As I worked through the aforementioned book, however, it was very clear that without the guiding instruction of the esteemed Dave Thomas and DHH I'd be up the creek without a paddle, and that got me thinking. Programming by convention is all great and frictionless and wonderful as long as you know the conventions . Imagine, if you will, the sheer blank incomprehension of a maintenance programmer who's never heard of Rails, sitting down to tweak a Rails application. Wait, what? How can this happen? Surely everyone has heard of Rails by now? Nope, sorry, but the truth is that the majority of programmers are clock- punchers living in a single-language world who don't read blogs, or play around with tech in their own time, and haven't even heard of Linux, let alone Rails. Their single language will likely be an everyday static language like Java or C#, which will leave them ill-prepared for many of the dynamic tricks in idiomatic Ruby. Ah, but surely the kind of forward-thinking proto-company that builds its product on RoR would never hire non-Ruby-savvy developers anyway? That might be the case if you drink the 37signals Kool-Aid and think that any RoR company is by default über-smart and infallible, but in the real world it doesn't work like that; there are countless tiny non-technical companies out there with just one or two developers - I know, because I spent a few years working at one - and maybe their current developers are cool enough to use RoR, but when they inevitably leave and the tech-illiterate management hire a replacement, you can guarantee that the job spec will not include minor details like \"must have at least heard of Ruby on Rails\". So, our imaginary maintenance guy - let's call him Ted - hired by a non- technical company to look after a web application, peers for the first time into the 37 directories (assuming no new ones have been added) and >45 files (since new ones will most certainly have been added), and nothing makes any sense. Even assuming Ted is smart enough to make reasonable guesses about Ruby syntax, and knows what MVC is, there's no visible link between the different layers of the application. It isn't clear how data is shuttled to and from the database. It isn't clear why things don't always work as expected when Ted tries to manually add new things, rather than using the Rails scripts (which he doesn't know about), even when diligently trying to emulate the structure and layout of existing code. It all seems like sorcery. What is Ted to do? The correct answer is to go and by a Rails book of course, or at least try and pick out the decent tutorials on the web (unfortunately, there's a lot more chaff than wheat in this area, maybe a sign that Rails is becoming a bit more mainstream?). A few days of getting up-to-speed, and Ted achieves enlightenment and becomes mega-productive, and lives happily ever after. So coding by convention is a good thing, right? Maybe. I'm still uneasy about sorcery, and Rails is some of the most effective sorcery I've seen. The main problem is that, well, it's sorcery. A couple of times in the 15 hours I spent going through Agile Web Development with Rails I hit problems. Not major ones, and always of my own making - some silly typo, or mistake coming from only having a working knowledge of Ruby rather than cosy familiarity. As is my wont, failure to spot the error after a cursory glance through the code led to a quick google search to see if I've hit a common problem, before resigning myself to going through the code in detail to sort it out (like all good programmers I'm a lazy devil). On these periodic google jaunts I found lots and lots of forum posts and blog entries from people who, and let's not mince words here, hadn't the first clue what they were doing. People that had heard the Rails hype, bought the book (and probably the t-shirt), hit problems, and were now running around like a cargo cult expecting magic spells to solve all their problems. Restart WEBrick. rake db:sessions:clear. Roll the most recent migration back then forward again. None of these work? Sorry, can't help. It reminds me of The IT Crowd 's \"have you tried turning it off and on again?\". I shouldn't be harsh on these folks; at least they're getting excited by Rails and are rolling up their sleeves and having a go, and no doubt some of them will succeed wildly and become far better, richer, more attractive programmers than I can ever hope to be. Also let me be clear that I think the productivity gains of software like Rails is a good thing, and Rails is certain to account for a good chunk of my tinkering time for the next few months. It worries me, however, when people try to run before they can walk, and the magic of coding by convention tends to encourage it. I'll leave it as an exercise for the reader to consider the implications of the fact that the sample application being conjured here by all these sorcerers' apprentices is an e-commerce site, at a time when online fraud is skyrocketing . I don't mean to single out Ruby on Rails specifically, by the way, it's just handy as an example due to its profile. Coding by convention is not new; if you want an older example of what happens when people are given programming tools that allow them to get something working - for fairly loose definitions of 'working' - without knowing much about what's happening under the hood, then look at the atrocities committed with VB and databinding over the years. Steve Yegge has a characteristically long and insightful rant on this subject, and is troubled by the difficulty of working out where to draw the line. The line, in this case, being the level of abstraction at which a programmer should understand a system - high enough not to be bogged down in insane detail (e.g. knowing how semiconductors work) but not so high that the role of programmer is reduced to that of sideshow conjurer, waving a cheap trick-shop wand and trusting to a higher power that everything will work out OK. Maybe it's just a generational disease. Maybe in ten years' time all the apprentices who have graduated to fully-fledged sorcerers will be looking on in dismay at the young scamps creating Web 5.0 applications using Ruby on MagLev simply by burping commands into their Skype headsets, and writing cautionary blogs about the dangers of not knowing how to write a partial web template. Theoretically, a perfect system - perhaps a descendant of Rails in the dim and distant future - would contain such exquisitely crafted assumptions and such frictionless conventions that it would never go wrong and always do the right thing. Thus, the need to understand anything at the lower level of abstraction required to sort out any problems is obviated, unless you are one of the very few Grand High Wizards who keep everything running smoothly. I don't know whether it's fortunate or unfortunate that such a system is unlikely to appear within my lifetime.","tags":"development","title":"Coding by Convention"},{"url":"reindenting-file-in-vim.html","text":"I'm going to post a series of helpful Vim snippets here, particularly for features that I don't necessarily use every day and hence forget about after a while. By posting them here, I've got a nice easy one-stop-shop for finding them. The first tip is reindenting source code. Hitting = will reindent visually- selected code , or you can also use a motion to constrain the affected area. gg=G will reindent the entire file: using System ; namespace IndentExample { public class Indent { public static void Main ( string [] args ) { Console . WriteLine ( \"badgerbadgerbadger\" ); } } } to using System ; namespace IndentExample { public class Indent { public static void Main ( string [] args ) { Console . WriteLine ( \"badgerbadgerbadger\" ); } } } More info: http://www.vim.org/tips/tip.php?tip_id=83","tags":"tools","title":"Reindenting a File in Vim"},{"url":"technical-book-club.html","text":"Back in October, personal finance blogger Trent at The Simple Dollar started an online book club for one of his favourite finance books. Good idea, I thought, so I'm nicking it. Starting in January, I'm running a technical book club at work with a few .Net devs, and I'll write everything up and post it here, so if you're so inclined you can follow along at home. To start with, we'll be reading language-agnostic books covering the fundamentals of software development in the real world, since it's always valuable to refresh knowledge on the cornerstones of modern professional coding; later on this can diversify into specific technologies and subjects with more arcane, academic, or abstract overtones. Another benefit of starting with the basics is that we can concentrate on getting the format right without feeling overwhelmed by unfamiliar material. So, the initial batch of texts will cover object-oriented design, design patterns, refactoring, code quality, and so on. Later, the idea is to study less immediate (but still vital) subjects like functional programming, compiler design, operating systems, etc.; and also to gain deeper knowledge of common specific technologies, e.g. the inner workings of the CLR or a JVM . I suspect people like stevey will argue that these latter subjects are more important than the others and should be done first, and they might even be right, but I've picked my approach and I'm sticking with it, so nuts to you stevey. So, here's the early schedule and probable books. The order we do these books might change - in fact the books themselves might change if, for example, we decide that Fowler's Enterprise Patterns is more appropriate than the GoF's Design Patterns . Each book will be agreed for certain in good time for it to be ordered and delivered before the scheduled start date, obviously. Topic Book Start Date OO Design Object-Oriented Analysis and Design with Applications 14/01/2008 Design Patterns Design Patterns 03/03/2008 Refactoring Refactoring 05/05/2008 Code Quality Pragmatic Programmer 07/07/2008 Legacy Code Working Effectively With Legacy Code 01/09/2008 In addition to these, we'll also cover one chapter of Code Complete per week. So, there it is. If you want to tag along, get yourself a copy of Booch's Object-Oriented Analysis and Design with Applications and McConnell's seminal Code Complete , and tune in next month.","tags":"bookclub","title":"Technical Book Club"}]}